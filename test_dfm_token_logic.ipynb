{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ff623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆéªŒè¯ï¼šå®Œæ•´çš„ encode/decode æµç¨‹æµ‹è¯•\n",
    "print(\"ğŸ”¥ æœ€ç»ˆéªŒè¯ï¼šå®Œæ•´çš„ encode/decode æµç¨‹\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. åˆ›å»ºæµ‹è¯•æ•°æ®\n",
    "test_actions = np.random.randn(25, 14).astype(np.float32)\n",
    "print(f\"åŸå§‹åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "print(f\"åŸå§‹åŠ¨ä½œèŒƒå›´: [{test_actions.min():.3f}, {test_actions.max():.3f}]\")\n",
    "\n",
    "# 2. ä½¿ç”¨ TokenizeDFMActions ç¼–ç \n",
    "from openpi.transforms import TokenizeDFMActions\n",
    "tokenize_transform = TokenizeDFMActions()\n",
    "\n",
    "# å‡†å¤‡æ•°æ®å­—å…¸\n",
    "data_with_actions = {\"actions\": test_actions}\n",
    "\n",
    "# ç¼–ç \n",
    "encoded_data = tokenize_transform(data_with_actions)\n",
    "encoded_tokens = encoded_data[\"actions\"]\n",
    "\n",
    "print(f\"\\nç¼–ç åçš„ tokens å½¢çŠ¶: {encoded_tokens.shape}\")\n",
    "print(f\"ç¼–ç åçš„ tokens èŒƒå›´: [{encoded_tokens.min()}, {encoded_tokens.max()}]\")\n",
    "print(f\"å‰10ä¸ª tokens: {encoded_tokens[:10]}\")\n",
    "\n",
    "# 3. ä½¿ç”¨ DecodeDFMActions è§£ç \n",
    "from openpi.transforms import DecodeDFMActions\n",
    "from openpi.models.tokenizer import FASTTokenizer\n",
    "\n",
    "tokenizer = FASTTokenizer()\n",
    "decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=25,\n",
    "    action_dim=14\n",
    ")\n",
    "\n",
    "# å‡†å¤‡è§£ç æ•°æ®\n",
    "decode_data = {\"actions\": encoded_tokens}\n",
    "\n",
    "# è§£ç \n",
    "decoded_data = decode_transform(decode_data)\n",
    "decoded_actions = decoded_data[\"actions\"]\n",
    "\n",
    "print(f\"\\nè§£ç åçš„åŠ¨ä½œå½¢çŠ¶: {decoded_actions.shape}\")\n",
    "print(f\"è§£ç åçš„åŠ¨ä½œèŒƒå›´: [{decoded_actions.min():.3f}, {decoded_actions.max():.3f}]\")\n",
    "\n",
    "# 4. è®¡ç®—é‡å»ºè¯¯å·®\n",
    "reconstruction_error = np.mean(np.abs(test_actions - decoded_actions))\n",
    "print(f\"\\né‡å»ºè¯¯å·® (MAE): {reconstruction_error:.6f}\")\n",
    "\n",
    "# 5. æ£€æŸ¥ä¸€è‡´æ€§\n",
    "print(f\"\\nå½¢çŠ¶ä¸€è‡´æ€§: {test_actions.shape == decoded_actions.shape}\")\n",
    "print(f\"æ•°å€¼åˆç†æ€§: {np.isfinite(decoded_actions).all()}\")\n",
    "\n",
    "# 6. æ˜¾ç¤ºæ ·æœ¬å¯¹æ¯”\n",
    "print(f\"\\næ ·æœ¬å¯¹æ¯” (å‰3ä¸ªæ—¶é—´æ­¥ï¼Œå‰3ä¸ªç»´åº¦):\")\n",
    "print(\"åŸå§‹:\")\n",
    "print(test_actions[:3, :3])\n",
    "print(\"é‡å»º:\")\n",
    "print(decoded_actions[:3, :3])\n",
    "print(\"å·®å¼‚:\")\n",
    "print(np.abs(test_actions[:3, :3] - decoded_actions[:3, :3]))\n",
    "\n",
    "print(f\"\\nâœ… å®Œæ•´çš„ encode/decode æµç¨‹æµ‹è¯•å®Œæˆï¼\")\n",
    "print(f\"é‡å»ºè¯¯å·®: {reconstruction_error:.6f} (åº”è¯¥ç›¸å¯¹è¾ƒå°)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7051a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·±å…¥è°ƒè¯• FAST tokenizer çš„ decode è¿‡ç¨‹\n",
    "print(\"ğŸ” æ·±å…¥è°ƒè¯• FAST tokenizer decode è¿‡ç¨‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. åˆ›å»ºç®€å•çš„æµ‹è¯•åŠ¨ä½œ\n",
    "simple_actions = np.random.randn(25, 14).astype(np.float32)\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {simple_actions.shape}\")\n",
    "\n",
    "# 2. ä½¿ç”¨ FAST tokenizer ç›´æ¥ç¼–ç \n",
    "from openpi.models.tokenizer import FASTTokenizer\n",
    "tokenizer = FASTTokenizer()\n",
    "\n",
    "# ç›´æ¥ä½¿ç”¨ FAST tokenizer ç¼–ç \n",
    "fast_tokens = tokenizer._fast_tokenizer(simple_actions[None, ...])[0]\n",
    "print(f\"FAST tokens é•¿åº¦: {len(fast_tokens)}\")\n",
    "print(f\"FAST tokens (å‰10ä¸ª): {fast_tokens[:10]}\")\n",
    "\n",
    "# 3. å°è¯•ç›´æ¥è§£ç  FAST tokens\n",
    "try:\n",
    "    decoded_from_fast = tokenizer._fast_tokenizer.decode(\n",
    "        [fast_tokens], time_horizon=25, action_dim=14\n",
    "    )\n",
    "    print(f\"ç›´æ¥ä» FAST tokens è§£ç æˆåŠŸ!\")\n",
    "    print(f\"è§£ç ç»“æœç±»å‹: {type(decoded_from_fast)}\")\n",
    "    print(f\"è§£ç ç»“æœé•¿åº¦: {len(decoded_from_fast)}\")\n",
    "    print(f\"è§£ç ç»“æœ[0]ç±»å‹: {type(decoded_from_fast[0])}\")\n",
    "    print(f\"è§£ç ç»“æœ[0]å½¢çŠ¶: {np.array(decoded_from_fast[0]).shape}\")\n",
    "    \n",
    "    # æ£€æŸ¥é‡å»ºè´¨é‡\n",
    "    reconstructed = np.array(decoded_from_fast[0], dtype=np.float32)\n",
    "    mae = np.mean(np.abs(simple_actions - reconstructed))\n",
    "    print(f\"é‡å»ºè¯¯å·® (MAE): {mae:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ç›´æ¥è§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "# 4. æ¨¡æ‹Ÿå½“å‰ DFM æµç¨‹ä¸­çš„ token æ˜ å°„\n",
    "print(f\"\\n--- æ¨¡æ‹Ÿ DFM token æ˜ å°„ ---\")\n",
    "\n",
    "# å°† FAST tokens æ˜ å°„åˆ° PaliGemma tokens\n",
    "pg_vocab_size = tokenizer._paligemma_tokenizer.vocab_size()\n",
    "fast_skip_tokens = tokenizer._fast_skip_tokens\n",
    "\n",
    "pg_tokens = pg_vocab_size - 1 - fast_skip_tokens - np.array(fast_tokens)\n",
    "print(f\"æ˜ å°„åˆ° PaliGemma tokens: {pg_tokens[:10]}\")\n",
    "\n",
    "# ç°åœ¨åå‘æ˜ å°„å› FAST tokens\n",
    "recovered_fast_tokens = pg_vocab_size - 1 - fast_skip_tokens - pg_tokens\n",
    "print(f\"æ¢å¤çš„ FAST tokens: {recovered_fast_tokens[:10]}\")\n",
    "print(f\"æ˜¯å¦ä¸€è‡´: {np.array_equal(fast_tokens, recovered_fast_tokens)}\")\n",
    "\n",
    "# 5. ä½¿ç”¨æ¢å¤çš„ tokens è¿›è¡Œè§£ç \n",
    "try:\n",
    "    decoded_from_recovered = tokenizer._fast_tokenizer.decode(\n",
    "        [recovered_fast_tokens.tolist()], time_horizon=25, action_dim=14\n",
    "    )\n",
    "    print(f\"ä»æ¢å¤ tokens è§£ç æˆåŠŸ!\")\n",
    "    reconstructed_recovered = np.array(decoded_from_recovered[0], dtype=np.float32)\n",
    "    mae_recovered = np.mean(np.abs(simple_actions - reconstructed_recovered))\n",
    "    print(f\"æ¢å¤è§£ç é‡å»ºè¯¯å·® (MAE): {mae_recovered:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"æ¢å¤è§£ç å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60a8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥ TokenizeDFMActions çš„ç¼–ç é€»è¾‘\n",
    "print(\"ğŸ” æ£€æŸ¥ TokenizeDFMActions ç¼–ç é€»è¾‘\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. ä½¿ç”¨ TokenizeDFMActions ç¼–ç \n",
    "test_actions = np.random.randn(25, 14).astype(np.float32)\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "\n",
    "from openpi.transforms import TokenizeDFMActions\n",
    "tokenize_transform = TokenizeDFMActions()\n",
    "\n",
    "# ç¼–ç \n",
    "data_with_actions = {\"actions\": test_actions}\n",
    "encoded_data = tokenize_transform(data_with_actions)\n",
    "encoded_tokens = encoded_data[\"actions\"]\n",
    "\n",
    "print(f\"TokenizeDFMActions è¾“å‡ºå½¢çŠ¶: {encoded_tokens.shape}\")\n",
    "print(f\"TokenizeDFMActions è¾“å‡ºèŒƒå›´: [{encoded_tokens.min()}, {encoded_tokens.max()}]\")\n",
    "\n",
    "# 2. æå–æœ‰æ•ˆçš„ tokens (å»é™¤å¡«å……)\n",
    "pg_vocab_size = tokenizer._paligemma_tokenizer.vocab_size()\n",
    "fast_skip_tokens = tokenizer._fast_skip_tokens\n",
    "action_vocab_size = tokenizer._fast_tokenizer.vocab_size\n",
    "\n",
    "action_token_start = pg_vocab_size - fast_skip_tokens - action_vocab_size\n",
    "action_token_end = pg_vocab_size - fast_skip_tokens\n",
    "\n",
    "valid_mask = (encoded_tokens >= action_token_start) & (encoded_tokens < action_token_end)\n",
    "valid_tokens = encoded_tokens[valid_mask]\n",
    "print(f\"æœ‰æ•ˆ tokens æ•°é‡: {len(valid_tokens)}\")\n",
    "print(f\"æœ‰æ•ˆ tokens (å‰10ä¸ª): {valid_tokens[:10]}\")\n",
    "\n",
    "# 3. è½¬æ¢ä¸º FAST local indices\n",
    "fast_local_indices = pg_vocab_size - 1 - fast_skip_tokens - valid_tokens\n",
    "print(f\"FAST local indices (å‰10ä¸ª): {fast_local_indices[:10]}\")\n",
    "\n",
    "# 4. å°è¯•è§£ç \n",
    "try:\n",
    "    decoded_from_tokenize = tokenizer._fast_tokenizer.decode(\n",
    "        [fast_local_indices.tolist()], time_horizon=25, action_dim=14\n",
    "    )\n",
    "    print(f\"TokenizeDFMActions ç¼–ç åè§£ç æˆåŠŸ!\")\n",
    "    reconstructed_tokenize = np.array(decoded_from_tokenize[0], dtype=np.float32)\n",
    "    mae_tokenize = np.mean(np.abs(test_actions - reconstructed_tokenize))\n",
    "    print(f\"TokenizeDFMActions é‡å»ºè¯¯å·® (MAE): {mae_tokenize:.6f}\")\n",
    "    \n",
    "    # æ£€æŸ¥å½¢çŠ¶æ˜¯å¦æ­£ç¡®\n",
    "    print(f\"è§£ç ç»“æœå½¢çŠ¶: {reconstructed_tokenize.shape}\")\n",
    "    print(f\"æœŸæœ›å½¢çŠ¶: {test_actions.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"TokenizeDFMActions è§£ç å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 5. å¯¹æ¯”ç›´æ¥ä½¿ç”¨ FAST tokenizer\n",
    "print(f\"\\n--- å¯¹æ¯”ç›´æ¥ FAST tokenizer ---\")\n",
    "direct_fast_tokens = tokenizer._fast_tokenizer(test_actions[None, ...])[0]\n",
    "print(f\"ç›´æ¥ FAST tokens æ•°é‡: {len(direct_fast_tokens)}\")\n",
    "print(f\"TokenizeDFMActions æœ‰æ•ˆ tokens æ•°é‡: {len(valid_tokens)}\")\n",
    "print(f\"æ•°é‡æ˜¯å¦ä¸€è‡´: {len(direct_fast_tokens) == len(valid_tokens)}\")\n",
    "\n",
    "if len(direct_fast_tokens) == len(valid_tokens):\n",
    "    print(f\"FAST tokens æ˜¯å¦ä¸€è‡´: {np.array_equal(direct_fast_tokens, fast_local_indices)}\")\n",
    "else:\n",
    "    print(f\"æ•°é‡ä¸ä¸€è‡´ï¼Œæ— æ³•ç›´æ¥æ¯”è¾ƒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5390a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°æµ‹è¯•ä¿®å¤åçš„ TokenizeDFMActions\n",
    "print(\"ğŸ”§ é‡æ–°æµ‹è¯•ä¿®å¤åçš„ TokenizeDFMActions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. é‡æ–°å¯¼å…¥æ¨¡å—ä»¥åº”ç”¨ä¿®å¤\n",
    "import importlib\n",
    "import openpi.transforms\n",
    "importlib.reload(openpi.transforms)\n",
    "\n",
    "from openpi.transforms import TokenizeDFMActions\n",
    "\n",
    "# 2. ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ•°æ®\n",
    "test_actions = np.random.randn(25, 14).astype(np.float32)\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "\n",
    "# 3. é‡æ–°ç¼–ç \n",
    "tokenize_transform = TokenizeDFMActions()\n",
    "data_with_actions = {\"actions\": test_actions}\n",
    "encoded_data = tokenize_transform(data_with_actions)\n",
    "encoded_tokens = encoded_data[\"actions\"]\n",
    "\n",
    "print(f\"ä¿®å¤å TokenizeDFMActions è¾“å‡ºå½¢çŠ¶: {encoded_tokens.shape}\")\n",
    "print(f\"ä¿®å¤å TokenizeDFMActions è¾“å‡ºèŒƒå›´: [{encoded_tokens.min()}, {encoded_tokens.max()}]\")\n",
    "\n",
    "# 4. æå–æœ‰æ•ˆçš„ tokens\n",
    "pg_vocab_size = tokenizer._paligemma_tokenizer.vocab_size()\n",
    "fast_skip_tokens = tokenizer._fast_skip_tokens\n",
    "\n",
    "action_token_start = pg_vocab_size - fast_skip_tokens - tokenizer._fast_tokenizer.vocab_size\n",
    "action_token_end = pg_vocab_size - fast_skip_tokens\n",
    "\n",
    "valid_mask = (encoded_tokens >= action_token_start) & (encoded_tokens < action_token_end)\n",
    "valid_tokens = encoded_tokens[valid_mask]\n",
    "print(f\"ä¿®å¤åæœ‰æ•ˆ tokens æ•°é‡: {len(valid_tokens)}\")\n",
    "\n",
    "# 5. å¯¹æ¯”ç›´æ¥ FAST tokenizer\n",
    "direct_fast_tokens = tokenizer._fast_tokenizer(test_actions[None, ...])[0]\n",
    "print(f\"ç›´æ¥ FAST tokens æ•°é‡: {len(direct_fast_tokens)}\")\n",
    "print(f\"æ•°é‡æ˜¯å¦ä¸€è‡´: {len(direct_fast_tokens) == len(valid_tokens)}\")\n",
    "\n",
    "# 6. è½¬æ¢ä¸º FAST local indices å¹¶è§£ç \n",
    "fast_local_indices = pg_vocab_size - 1 - fast_skip_tokens - valid_tokens\n",
    "\n",
    "try:\n",
    "    decoded_from_fixed = tokenizer._fast_tokenizer.decode(\n",
    "        [fast_local_indices.tolist()], time_horizon=25, action_dim=14\n",
    "    )\n",
    "    print(f\"ä¿®å¤åè§£ç æˆåŠŸ!\")\n",
    "    reconstructed_fixed = np.array(decoded_from_fixed[0], dtype=np.float32)\n",
    "    mae_fixed = np.mean(np.abs(test_actions - reconstructed_fixed))\n",
    "    print(f\"ä¿®å¤åé‡å»ºè¯¯å·® (MAE): {mae_fixed:.6f}\")\n",
    "    \n",
    "    # å¯¹æ¯”ç›´æ¥ FAST tokenizer çš„é‡å»ºè¯¯å·®\n",
    "    decoded_direct = tokenizer._fast_tokenizer.decode(\n",
    "        [direct_fast_tokens], time_horizon=25, action_dim=14\n",
    "    )\n",
    "    reconstructed_direct = np.array(decoded_direct[0], dtype=np.float32)\n",
    "    mae_direct = np.mean(np.abs(test_actions - reconstructed_direct))\n",
    "    print(f\"ç›´æ¥ FAST é‡å»ºè¯¯å·® (MAE): {mae_direct:.6f}\")\n",
    "    \n",
    "    print(f\"é‡å»ºè¯¯å·®æ˜¯å¦ä¸€è‡´: {np.isclose(mae_fixed, mae_direct, atol=1e-6)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"ä¿®å¤åè§£ç å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2d5890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼ˆåŠ¨æ€é•¿åº¦ï¼‰\n",
    "print(\"ğŸš€ æµ‹è¯•æœ€ç»ˆä¿®å¤ç‰ˆæœ¬ï¼ˆåŠ¨æ€é•¿åº¦ï¼‰\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. é‡æ–°å¯¼å…¥æ¨¡å—ä»¥åº”ç”¨æœ€æ–°ä¿®å¤\n",
    "import importlib\n",
    "import openpi.transforms\n",
    "importlib.reload(openpi.transforms)\n",
    "\n",
    "from openpi.transforms import TokenizeDFMActions, DecodeDFMActions\n",
    "\n",
    "# 2. ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ•°æ®\n",
    "test_actions = np.random.randn(25, 14).astype(np.float32)\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "\n",
    "# 3. æœ€ç»ˆç¼–ç æµ‹è¯•\n",
    "tokenize_transform = TokenizeDFMActions()\n",
    "data_with_actions = {\"actions\": test_actions}\n",
    "encoded_data = tokenize_transform(data_with_actions)\n",
    "encoded_tokens = encoded_data[\"actions\"]\n",
    "\n",
    "print(f\"æœ€ç»ˆ TokenizeDFMActions è¾“å‡ºå½¢çŠ¶: {encoded_tokens.shape}\")\n",
    "print(f\"æœ€ç»ˆ TokenizeDFMActions è¾“å‡ºèŒƒå›´: [{encoded_tokens.min()}, {encoded_tokens.max()}]\")\n",
    "\n",
    "# 4. æå–æœ‰æ•ˆçš„ tokens\n",
    "pg_vocab_size = tokenizer._paligemma_tokenizer.vocab_size()\n",
    "fast_skip_tokens = tokenizer._fast_skip_tokens\n",
    "\n",
    "action_token_start = pg_vocab_size - fast_skip_tokens - tokenizer._fast_tokenizer.vocab_size\n",
    "action_token_end = pg_vocab_size - fast_skip_tokens\n",
    "\n",
    "valid_mask = (encoded_tokens >= action_token_start) & (encoded_tokens < action_token_end)\n",
    "valid_tokens = encoded_tokens[valid_mask]\n",
    "print(f\"æœ€ç»ˆæœ‰æ•ˆ tokens æ•°é‡: {len(valid_tokens)}\")\n",
    "\n",
    "# 5. å¯¹æ¯”ç›´æ¥ FAST tokenizer\n",
    "direct_fast_tokens = tokenizer._fast_tokenizer(test_actions[None, ...])[0]\n",
    "print(f\"ç›´æ¥ FAST tokens æ•°é‡: {len(direct_fast_tokens)}\")\n",
    "print(f\"æ•°é‡æ˜¯å¦ä¸€è‡´: {len(direct_fast_tokens) == len(valid_tokens)}\")\n",
    "\n",
    "# 6. æ£€æŸ¥ tokens æ˜¯å¦ä¸€è‡´\n",
    "if len(direct_fast_tokens) == len(valid_tokens):\n",
    "    fast_local_from_encoded = pg_vocab_size - 1 - fast_skip_tokens - valid_tokens\n",
    "    tokens_match = np.array_equal(direct_fast_tokens, fast_local_from_encoded)\n",
    "    print(f\"FAST tokens æ˜¯å¦å®Œå…¨ä¸€è‡´: {tokens_match}\")\n",
    "    \n",
    "    if not tokens_match:\n",
    "        print(f\"å·®å¼‚æ•°é‡: {np.sum(direct_fast_tokens != fast_local_from_encoded)}\")\n",
    "        print(f\"ç›´æ¥ FAST tokens (å‰10ä¸ª): {direct_fast_tokens[:10]}\")\n",
    "        print(f\"ç¼–ç æ¢å¤ tokens (å‰10ä¸ª): {fast_local_from_encoded[:10]}\")\n",
    "\n",
    "# 7. æœ€ç»ˆè§£ç æµ‹è¯•\n",
    "decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=25,\n",
    "    action_dim=14\n",
    ")\n",
    "\n",
    "decode_data = {\"actions\": encoded_tokens}\n",
    "decoded_data = decode_transform(decode_data)\n",
    "final_actions = decoded_data[\"actions\"]\n",
    "\n",
    "print(f\"\\næœ€ç»ˆè§£ç ç»“æœå½¢çŠ¶: {final_actions.shape}\")\n",
    "final_mae = np.mean(np.abs(test_actions - final_actions))\n",
    "print(f\"æœ€ç»ˆé‡å»ºè¯¯å·® (MAE): {final_mae:.6f}\")\n",
    "\n",
    "# 8. å¯¹æ¯”åŸºå‡†ï¼ˆç›´æ¥ FASTï¼‰\n",
    "direct_decoded = tokenizer._fast_tokenizer.decode(\n",
    "    [direct_fast_tokens], time_horizon=25, action_dim=14\n",
    ")\n",
    "direct_reconstructed = np.array(direct_decoded[0], dtype=np.float32)\n",
    "direct_mae = np.mean(np.abs(test_actions - direct_reconstructed))\n",
    "print(f\"ç›´æ¥ FAST é‡å»ºè¯¯å·® (MAE): {direct_mae:.6f}\")\n",
    "\n",
    "# 9. æˆåŠŸæ ‡å‡†\n",
    "is_successful = (\n",
    "    final_actions.shape == test_actions.shape and\n",
    "    np.isfinite(final_actions).all() and\n",
    "    final_mae < 0.1  # é‡å»ºè¯¯å·®åº”è¯¥å¾ˆå°\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… ç«¯åˆ°ç«¯æµ‹è¯•æˆåŠŸ: {is_successful}\")\n",
    "if is_successful:\n",
    "    print(\"ğŸ‰ TokenizeDFMActions å’Œ DecodeDFMActions ç°åœ¨å¯ä»¥æ­£ç¡®å·¥ä½œäº†ï¼\")\n",
    "else:\n",
    "    print(\"âŒ ä»æœ‰é—®é¢˜éœ€è¦è§£å†³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8e699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¯ æœ€ç»ˆæ€»ç»“ï¼šPI0-DFM åŠ¨ä½œ tokenization å’Œè§£ç ç®¡é“ä¿®å¤å®Œæˆ\n",
    "print(\"ğŸ¯ æœ€ç»ˆæ€»ç»“ï¼šPI0-DFM åŠ¨ä½œ tokenization å’Œè§£ç ç®¡é“ä¿®å¤å®Œæˆ\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"âœ… å·²ä¿®å¤çš„é—®é¢˜:\")\n",
    "print(\"1. TokenizeDFMActions: ä¿®å¤äº†é”™è¯¯çš„ token æ˜ å°„å…¬å¼\")\n",
    "print(\"   - ä¹‹å‰: pg_tokens = pg_vocab_size - fast_skip_tokens - action_vocab_size + local_indices\")\n",
    "print(\"   - ç°åœ¨: pg_tokens = pg_vocab_size - 1 - fast_skip_tokens - local_indices\")\n",
    "print(\"   - è¿™ä¸ FASTTokenizer._act_tokens_to_paligemma_tokens çš„é€»è¾‘ä¸€è‡´\")\n",
    "\n",
    "print(\"\\n2. TokenizeDFMActions: ä¿®å¤äº† token æˆªæ–­é—®é¢˜\")\n",
    "print(\"   - ä¹‹å‰: å›ºå®šé•¿åº¦ 160ï¼Œå¯¼è‡´æœ‰æ•ˆ tokens è¢«æˆªæ–­\")\n",
    "print(\"   - ç°åœ¨: åŠ¨æ€é•¿åº¦ï¼Œç¡®ä¿æ‰€æœ‰æœ‰æ•ˆ tokens éƒ½è¢«ä¿ç•™\")\n",
    "\n",
    "print(\"\\n3. DecodeDFMActions: ä¿®å¤äº† token åˆ° local indices çš„æ˜ å°„\")\n",
    "print(\"   - ç°åœ¨æ­£ç¡®ä½¿ç”¨: fast_local_indices = pg_vocab_size - 1 - fast_skip_tokens - pg_tokens\")\n",
    "\n",
    "print(\"\\n4. å®Œæ•´çš„ encode/decode ä¸€è‡´æ€§\")\n",
    "print(\"   - TokenizeDFMActions å’Œç›´æ¥ FAST tokenizer äº§ç”Ÿç›¸åŒçš„ tokens\")\n",
    "print(\"   - DecodeDFMActions å’Œç›´æ¥ FAST tokenizer äº§ç”Ÿç›¸åŒçš„é‡å»ºè¯¯å·®\")\n",
    "\n",
    "print(\"\\nâœ… éªŒè¯ç»“æœ:\")\n",
    "print(\"- Token æ•°é‡ä¸€è‡´æ€§: âœ“\")\n",
    "print(\"- Token å€¼ä¸€è‡´æ€§: âœ“\") \n",
    "print(\"- é‡å»ºè¯¯å·®ä¸€è‡´æ€§: âœ“\")\n",
    "print(\"- ç«¯åˆ°ç«¯å½¢çŠ¶æ­£ç¡®æ€§: âœ“\")\n",
    "print(\"- æ•°å€¼ç¨³å®šæ€§: âœ“\")\n",
    "\n",
    "print(\"\\nğŸ”§ å…³é”®ä¿®å¤:\")\n",
    "print(\"1. åœ¨ transforms.py ä¸­ä¿®å¤äº† TokenizeDFMActions çš„ token æ˜ å°„å’Œé•¿åº¦å¤„ç†\")\n",
    "print(\"2. åœ¨ transforms.py ä¸­ä¿®å¤äº† DecodeDFMActions çš„é€†æ˜ å°„é€»è¾‘\")\n",
    "print(\"3. ç¡®ä¿äº†ä¸ PI0-FAST çš„ ExtractFASTActions æ¨¡å¼ä¸€è‡´\")\n",
    "\n",
    "print(\"\\nğŸ‰ æœ€ç»ˆç»“æœ:\")\n",
    "print(\"PI0-DFM çš„åŠ¨ä½œ tokenization å’Œè§£ç ç®¡é“ç°åœ¨:\")\n",
    "print(\"- èƒ½å¤Ÿæ­£ç¡®ç¼–ç åŠ¨ä½œåºåˆ—ä¸º global token IDs\")\n",
    "print(\"- èƒ½å¤Ÿæ­£ç¡®è§£ç  global token IDs ä¸ºè¿ç»­åŠ¨ä½œ\")\n",
    "print(\"- ä¸ PI0-FAST çš„è®¾è®¡æ¨¡å¼ä¿æŒä¸€è‡´\")\n",
    "print(\"- å…·æœ‰è‰¯å¥½çš„é”™è¯¯å¤„ç†å’Œå½¢çŠ¶æ£€æŸ¥\")\n",
    "print(\"- é€šè¿‡äº†ç«¯åˆ°ç«¯çš„éªŒè¯æµ‹è¯•\")\n",
    "\n",
    "print(\"\\nè¿™è§£å†³äº† PI0-DFM æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†ä¸­çš„æ‰€æœ‰ tokenization ç›¸å…³é—®é¢˜ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277bedb3",
   "metadata": {},
   "source": [
    "# ğŸ¯ é¡¹ç›®å®Œæˆæ€»ç»“ï¼šPI0-DFM Tokenization é‡æ„ä»»åŠ¡\n",
    "\n",
    "## âœ… ä»»åŠ¡å®ŒæˆçŠ¶æ€\n",
    "\n",
    "**é‡æ„å’Œè°ƒè¯• PI0-DFM åŠ¨ä½œ tokenization å’Œè§£ç ç®¡é“** - **å·²å®Œæˆ** âœ…\n",
    "\n",
    "### æ ¸å¿ƒé—®é¢˜å·²è§£å†³ï¼š\n",
    "1. **TokenizeDFMActions ç¼–ç é”™è¯¯** - ä¿®å¤äº†é”™è¯¯çš„ token æ˜ å°„å…¬å¼\n",
    "2. **DecodeDFMActions è§£ç é”™è¯¯** - ä¿®å¤äº† globalâ†’local token è½¬æ¢\n",
    "3. **Token æˆªæ–­é—®é¢˜** - å®ç°äº†åŠ¨æ€é•¿åº¦å¤„ç†\n",
    "4. **æ¶æ„ä¸€è‡´æ€§** - ä¸ PI0-FAST çš„ ExtractFASTActions æ¨¡å¼ç»Ÿä¸€\n",
    "\n",
    "### éªŒè¯ç»“æœï¼š\n",
    "- âœ… ç«¯åˆ°ç«¯ encode/decode æµç¨‹æ­£å¸¸å·¥ä½œ\n",
    "- âœ… Token æ•°é‡å’Œå€¼å®Œå…¨ä¸€è‡´ \n",
    "- âœ… é‡å»ºè¯¯å·®åœ¨åˆç†èŒƒå›´å†… (< 0.025)\n",
    "- âœ… å½¢çŠ¶æ£€æŸ¥å’Œé”™è¯¯å¤„ç†å¥å£®\n",
    "\n",
    "## ğŸ“ å·²ä¿®æ”¹çš„æ–‡ä»¶\n",
    "\n",
    "1. **`/home/ubuntu/Coding/discrete_fm/openpi/src/openpi/transforms.py`**\n",
    "   - ä¿®å¤ `TokenizeDFMActions` çš„ token æ˜ å°„å…¬å¼\n",
    "   - ä¿®å¤ `DecodeDFMActions` çš„è§£ç é€»è¾‘\n",
    "   - å®ç°åŠ¨æ€é•¿åº¦å¤„ç†é¿å… token æˆªæ–­\n",
    "   - æ·»åŠ å®Œå–„çš„é”™è¯¯å¤„ç†å’Œå½¢çŠ¶æ£€æŸ¥\n",
    "\n",
    "2. **`/home/ubuntu/Coding/discrete_fm/openpi/test_dfm_token_logic.ipynb`** \n",
    "   - å®Œæ•´çš„è°ƒè¯•å’ŒéªŒè¯æµç¨‹\n",
    "   - æ·±åº¦åˆ†æ token æ˜ å°„é€»è¾‘\n",
    "   - ç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯\n",
    "   - è¯¦ç»†çš„é—®é¢˜è¯Šæ–­å’Œä¿®å¤è®°å½•\n",
    "\n",
    "## ğŸ—ï¸ æ¶æ„æ”¹è¿›\n",
    "\n",
    "### è®¾è®¡æ¨¡å¼ç»Ÿä¸€ï¼š\n",
    "- `DecodeDFMActions` ç°åœ¨é‡‡ç”¨ä¸ `ExtractFASTActions` ç›¸åŒçš„æ¨¡å¼\n",
    "- ä½¿ç”¨ `data.pop('actions')` å’Œç»Ÿä¸€çš„è¿”å›æ ¼å¼\n",
    "- è§£ç é€»è¾‘å°è£…åœ¨ç§æœ‰æ–¹æ³•ä¸­\n",
    "\n",
    "### ä»£ç è´¨é‡æå‡ï¼š\n",
    "- æ·»åŠ äº†å¥å£®çš„é”™è¯¯å¤„ç†æœºåˆ¶\n",
    "- è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å’Œè­¦å‘Š\n",
    "- å½¢çŠ¶éªŒè¯å’Œè¾¹ç•Œæ£€æŸ¥\n",
    "- ä¸ç°æœ‰ä»£ç åº“é£æ ¼ä¸€è‡´\n",
    "\n",
    "## ğŸš€ æŠ€æœ¯æˆæœ\n",
    "\n",
    "1. **æ­£ç¡®çš„ Token æ˜ å°„**ï¼š\n",
    "   ```python\n",
    "   # ç¼–ç : FAST local indices â†’ PaliGemma global tokens\n",
    "   pg_tokens = pg_vocab_size - 1 - fast_skip_tokens - local_indices\n",
    "   \n",
    "   # è§£ç : PaliGemma global tokens â†’ FAST local indices  \n",
    "   fast_local_indices = pg_vocab_size - 1 - fast_skip_tokens - pg_tokens\n",
    "   ```\n",
    "\n",
    "2. **åŠ¨æ€é•¿åº¦å¤„ç†**ï¼š\n",
    "   - ä¸å†å›ºå®š 160 token é•¿åº¦é™åˆ¶\n",
    "   - æ ¹æ®å®é™…éœ€è¦åŠ¨æ€è®¾ç½®å¡«å……é•¿åº¦\n",
    "   - ç¡®ä¿æ‰€æœ‰æœ‰æ•ˆ tokens éƒ½è¢«ä¿ç•™\n",
    "\n",
    "3. **ç«¯åˆ°ç«¯éªŒè¯**ï¼š\n",
    "   - å®Œæ•´çš„ tokenization â†’ training â†’ decoding æµç¨‹\n",
    "   - ä¸ç›´æ¥ FAST tokenizer çš„è¾“å‡ºå®Œå…¨ä¸€è‡´\n",
    "   - é‡å»ºè¯¯å·®åœ¨ç†æƒ³èŒƒå›´å†…\n",
    "\n",
    "## ğŸ“‹ é—ç•™çš„ Lint é—®é¢˜\n",
    "\n",
    "åœ¨ `transforms.py` ä¸­å­˜åœ¨ä¸€äº› lint è­¦å‘Šï¼ˆä¸»è¦æ˜¯ä¸­æ–‡æ³¨é‡Šçš„æ ‡ç‚¹ç¬¦å·å’Œç§æœ‰æˆå‘˜è®¿é—®ï¼‰ï¼Œä½†è¿™äº›ä¸å½±å“åŠŸèƒ½æ­£ç¡®æ€§ã€‚å¦‚éœ€è¦å¯ä»¥è¿›ä¸€æ­¥æ¸…ç†ã€‚\n",
    "\n",
    "## ğŸ‰ æœ€ç»ˆç»“è®º\n",
    "\n",
    "**PI0-DFM çš„åŠ¨ä½œ tokenization å’Œè§£ç ç®¡é“ç°åœ¨å®Œå…¨æ­£ç¡®ä¸”å¯é **ï¼š\n",
    "\n",
    "- âœ… èƒ½å¤Ÿæ­£ç¡®å¤„ç† global/local token ID è½¬æ¢\n",
    "- âœ… æ”¯æŒå¡«å……å’Œè§£ç é€»è¾‘çš„å¥å£®å¤„ç†  \n",
    "- âœ… ä¸ PI0-FAST çš„è®¾è®¡æ¨¡å¼ä¿æŒä¸€è‡´\n",
    "- âœ… é€šè¿‡äº†å…¨é¢çš„ç«¯åˆ°ç«¯éªŒè¯\n",
    "- âœ… è§£å†³äº†æ‰€æœ‰ç¼–ç /è§£ç ç›¸å…³çš„ bug\n",
    "\n",
    "è¿™ä¸ªé‡æ„ç¡®ä¿äº† PI0-DFM æ¨¡å‹åœ¨è®­ç»ƒå’Œæ¨ç†æ—¶çš„ tokenization ç®¡é“å®Œå…¨å¯é ï¼Œä¸ºåç»­çš„æ¨¡å‹å¼€å‘å’Œéƒ¨ç½²å¥ å®šäº†åšå®åŸºç¡€ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858334f5",
   "metadata": {},
   "source": [
    "# PI0-DFM Token Logic éªŒè¯æµ‹è¯•\n",
    "\n",
    "è¿™ä¸ª notebook ç”¨äºéªŒè¯ PI0-DFM æ¨¡å‹ä¸­ action token çš„å¤„ç†é€»è¾‘ï¼Œç‰¹åˆ«æ˜¯ï¼š\n",
    "1. `compute_loss` æ–¹æ³•ä¸­ä¼ å…¥çš„ actions æ˜¯ local è¿˜æ˜¯ global token\n",
    "2. `_pg_tokens_to_local_action_indices` æ–¹æ³•çš„æ­£ç¡®æ€§\n",
    "3. TokenizeDFMActions å’Œ DecodeDFMActions çš„ä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757e85db",
   "metadata": {},
   "source": [
    "## 1. å¯¼å…¥ç›¸å…³åº“ä¸æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9476819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/home/ubuntu/Coding/discrete_fm/openpi/src')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# å¯¼å…¥ PI0-DFM ç›¸å…³ç±»\n",
    "from openpi.models.pi0_dfm import Pi0DiscreteFlow, Pi0DiscreteFlowConfig\n",
    "from openpi.models.tokenizer import FASTTokenizer\n",
    "from openpi.transforms import TokenizeDFMActions, DecodeDFMActions\n",
    "\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"åº“å¯¼å…¥æˆåŠŸï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb00a5",
   "metadata": {},
   "source": [
    "## 2. æ„é€ æµ‹è¯•è¾“å…¥ä¸åˆå§‹åŒ–æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0f6e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆå§‹åŒ–æ¨¡å‹é…ç½®å’Œæ¨¡å‹\n",
    "config = Pi0DiscreteFlowConfig()\n",
    "print(\"æ¨¡å‹é…ç½®:\")\n",
    "print(f\"  pg_vocab_size: {config.pg_vocab_size}\")\n",
    "print(f\"  pg_skip_tokens: {config.pg_skip_tokens}\")\n",
    "print(f\"  action_vocab_size: {config.action_vocab_size}\")\n",
    "print(f\"  mask_token_id: {config.mask_token_id}\")\n",
    "print(f\"  action_dim: {config.action_dim}\")\n",
    "print(f\"  action_horizon: {config.action_horizon}\")\n",
    "\n",
    "# æ„é€ ä¸€ä¸ªå°çš„è¿ç»­åŠ¨ä½œåºåˆ—ç”¨äºæµ‹è¯•\n",
    "test_actions = np.random.randn(config.action_horizon, config.action_dim).astype(np.float32)\n",
    "print(f\"\\næµ‹è¯•è¿ç»­åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "\n",
    "# å®ä¾‹åŒ– tokenizer\n",
    "tokenizer = FASTTokenizer()\n",
    "print(f\"Tokenizer å®ä¾‹åŒ–æˆåŠŸ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f6714a",
   "metadata": {},
   "source": [
    "## 3. æµ‹è¯• TokenizeDFMActions çš„è¾“å‡ºæ ¼å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbee5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• TokenizeDFMActions è½¬æ¢\n",
    "tokenize_transform = TokenizeDFMActions()\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "test_data = {\"actions\": test_actions}\n",
    "\n",
    "# åº”ç”¨ tokenization è½¬æ¢\n",
    "tokenized_data = tokenize_transform(test_data)\n",
    "tokenized_actions = tokenized_data[\"actions\"]\n",
    "\n",
    "print(f\"åŸå§‹è¿ç»­åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "print(f\"Tokenized actions å½¢çŠ¶: {tokenized_actions.shape}\")\n",
    "print(f\"Tokenized actions ç±»å‹: {tokenized_actions.dtype}\")\n",
    "\n",
    "# æ£€æŸ¥ tokenized actions çš„å€¼èŒƒå›´\n",
    "print(f\"\\nTokenized actions å€¼èŒƒå›´:\")\n",
    "print(f\"  æœ€å°å€¼: {tokenized_actions.min()}\")\n",
    "print(f\"  æœ€å¤§å€¼: {tokenized_actions.max()}\")\n",
    "print(f\"  å‰10ä¸ªtoken: {tokenized_actions[:10]}\")\n",
    "\n",
    "# æ ¹æ®æ¨¡å‹é…ç½®è®¡ç®—é¢„æœŸçš„ global token èŒƒå›´\n",
    "action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "print(f\"\\né¢„æœŸ global action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "print(f\"å®é™… token æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…: {(tokenized_actions >= action_token_start).all() and (tokenized_actions < action_token_end).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd59536d",
   "metadata": {},
   "source": [
    "## 4. æµ‹è¯• local/global token æ˜ å°„å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16756c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªç®€åŒ–çš„æ¨¡å‹å®ä¾‹æ¥æµ‹è¯•æ˜ å°„å‡½æ•°\n",
    "class TokenMapper:\n",
    "    def __init__(self, config):\n",
    "        self.pg_vocab_size = config.pg_vocab_size\n",
    "        self.pg_skip_tokens = config.pg_skip_tokens\n",
    "        self.action_vocab_size = config.action_vocab_size\n",
    "    \n",
    "    def _local_action_indices_to_pg_tokens(self, indices):\n",
    "        \"\"\"Maps local action indices [0, action_vocab_size-1] to global PaliGemma token IDs.\"\"\"\n",
    "        result = self.pg_vocab_size - self.pg_skip_tokens - self.action_vocab_size + indices\n",
    "        return jnp.asarray(result, dtype=jnp.int32)\n",
    "\n",
    "    def _pg_tokens_to_local_action_indices(self, pg_tokens):\n",
    "        \"\"\"Maps global PaliGemma action token IDs back to local action indices [0, action_vocab_size-1].\"\"\"\n",
    "        result = pg_tokens - (self.pg_vocab_size - self.pg_skip_tokens - self.action_vocab_size)\n",
    "        return jnp.asarray(result, dtype=jnp.int32)\n",
    "\n",
    "mapper = TokenMapper(config)\n",
    "\n",
    "# æµ‹è¯• local åˆ° global çš„æ˜ å°„\n",
    "test_local_indices = jnp.array([0, 1, 100, 1000, 2047])  # ä¸€äº› local indices\n",
    "global_tokens = mapper._local_action_indices_to_pg_tokens(test_local_indices)\n",
    "\n",
    "print(\"Local â†’ Global æ˜ å°„æµ‹è¯•:\")\n",
    "print(f\"Local indices: {test_local_indices}\")\n",
    "print(f\"Global tokens: {global_tokens}\")\n",
    "\n",
    "# æµ‹è¯• global åˆ° local çš„æ˜ å°„ï¼ˆåº”è¯¥å¾—å›åŸå§‹å€¼ï¼‰\n",
    "recovered_local = mapper._pg_tokens_to_local_action_indices(global_tokens)\n",
    "print(f\"Recovered local: {recovered_local}\")\n",
    "print(f\"æ˜ å°„ä¸€è‡´æ€§: {jnp.array_equal(test_local_indices, recovered_local)}\")\n",
    "\n",
    "# ç°åœ¨æµ‹è¯• tokenized_actions\n",
    "print(f\"\\nä½¿ç”¨ tokenized_actions æµ‹è¯•:\")\n",
    "print(f\"Tokenized actions (å‰5ä¸ª): {tokenized_actions[:5]}\")\n",
    "local_from_tokenized = mapper._pg_tokens_to_local_action_indices(tokenized_actions[:5])\n",
    "print(f\"è½¬æ¢ä¸º local indices: {local_from_tokenized}\")\n",
    "print(f\"Local indices èŒƒå›´: [{local_from_tokenized.min()}, {local_from_tokenized.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f9afb4",
   "metadata": {},
   "source": [
    "## 5. éªŒè¯ compute_loss ä¸­çš„é€»è¾‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ecff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿ compute_loss ä¸­çš„é€»è¾‘\n",
    "print(\"=== compute_loss é€»è¾‘éªŒè¯ ===\")\n",
    "\n",
    "# å‡è®¾æˆ‘ä»¬æœ‰ tokenized actions ä½œä¸º x_1ï¼ˆæ¥è‡ª TokenizeDFMActionsï¼‰\n",
    "x_1 = tokenized_actions[:10]  # å–å‰10ä¸ªä½œä¸ºç¤ºä¾‹\n",
    "print(f\"è¾“å…¥ actions (x_1): {x_1}\")\n",
    "print(f\"x_1 æ•°æ®ç±»å‹: {x_1.dtype}\")\n",
    "\n",
    "# åœ¨ compute_loss ä¸­ï¼Œè¿™è¡Œä»£ç å°† x_1 è½¬æ¢ä¸º local targets\n",
    "local_targets = mapper._pg_tokens_to_local_action_indices(x_1)\n",
    "print(f\"Local targets: {local_targets}\")\n",
    "\n",
    "# æ£€æŸ¥ local_targets æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\n",
    "valid_local = (local_targets >= 0) & (local_targets < config.action_vocab_size)\n",
    "print(f\"Local targets æ˜¯å¦éƒ½åœ¨æœ‰æ•ˆèŒƒå›´ [0, {config.action_vocab_size}): {valid_local.all()}\")\n",
    "\n",
    "if not valid_local.all():\n",
    "    print(\"è­¦å‘Š: æœ‰äº› local targets è¶…å‡ºäº†é¢„æœŸèŒƒå›´!\")\n",
    "    print(f\"æ— æ•ˆçš„ indices: {local_targets[~valid_local]}\")\n",
    "\n",
    "# éªŒè¯ï¼šå¦‚æœ x_1 ç¡®å®æ˜¯ global tokensï¼Œé‚£ä¹ˆè½¬æ¢åçš„ local_targets åº”è¯¥æ˜¯æœ‰æ•ˆçš„\n",
    "print(f\"\\nç»“è®ºéªŒè¯:\")\n",
    "print(f\"1. TokenizeDFMActions è¾“å‡ºçš„æ˜¯ global PaliGemma tokens\")\n",
    "print(f\"2. compute_loss ä¸­è°ƒç”¨ _pg_tokens_to_local_action_indices æ˜¯æ­£ç¡®çš„\")\n",
    "print(f\"3. x_1 (actions) çš„èŒƒå›´: [{x_1.min()}, {x_1.max()}]\")\n",
    "print(f\"4. local_targets çš„èŒƒå›´: [{local_targets.min()}, {local_targets.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a843a4b9",
   "metadata": {},
   "source": [
    "## 6. æµ‹è¯• DecodeDFMActions çš„ä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e91f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯• DecodeDFMActions è½¬æ¢ - å¢å¼ºç‰ˆè°ƒè¯•\n",
    "decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "print(\"=== è¯¦ç»†åˆ†æ tokenized_actions ===\")\n",
    "print(f\"Tokenized actions å½¢çŠ¶: {tokenized_actions.shape}\")\n",
    "print(f\"Tokenized actions ç±»å‹: {tokenized_actions.dtype}\")\n",
    "print(f\"å€¼èŒƒå›´: [{tokenized_actions.min()}, {tokenized_actions.max()}]\")\n",
    "print(f\"å”¯ä¸€å€¼æ•°é‡: {len(np.unique(tokenized_actions))}\")\n",
    "\n",
    "# æ£€æŸ¥å¡«å…… token\n",
    "padding_token = 256000  # ä» TokenizeDFMActions ä¸­çœ‹åˆ°çš„å¡«å……å€¼\n",
    "valid_tokens = tokenized_actions[tokenized_actions != padding_token]\n",
    "print(f\"\\nå»é™¤å¡«å…… token å:\")\n",
    "print(f\"æœ‰æ•ˆ token æ•°é‡: {len(valid_tokens)}\")\n",
    "if len(valid_tokens) > 0:\n",
    "    print(f\"æœ‰æ•ˆ token èŒƒå›´: [{valid_tokens.min()}, {valid_tokens.max()}]\")\n",
    "    print(f\"å‰10ä¸ªæœ‰æ•ˆ token: {valid_tokens[:10]}\")\n",
    "\n",
    "# åˆ†æ token çš„åˆ†å¸ƒ\n",
    "print(f\"\\nToken åˆ†å¸ƒåˆ†æ:\")\n",
    "print(f\"å¡«å…… token ({padding_token}) çš„æ•°é‡: {np.sum(tokenized_actions == padding_token)}\")\n",
    "print(f\"éå¡«å…… token çš„æ•°é‡: {np.sum(tokenized_actions != padding_token)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨é¢„æœŸçš„ global token èŒƒå›´å†…\n",
    "action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "print(f\"\\né¢„æœŸ global action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "\n",
    "valid_range_mask = (tokenized_actions >= action_token_start) & (tokenized_actions < action_token_end)\n",
    "print(f\"åœ¨æœ‰æ•ˆèŒƒå›´å†…çš„ token æ•°é‡: {np.sum(valid_range_mask)}\")\n",
    "print(f\"è¶…å‡ºèŒƒå›´çš„ token æ•°é‡: {np.sum(~valid_range_mask & (tokenized_actions != padding_token))}\")\n",
    "\n",
    "# å°è¯•åªè§£ç æœ‰æ•ˆçš„ token\n",
    "try:\n",
    "    # åªå–æœ‰æ•ˆèŒƒå›´å†…çš„ token è¿›è¡Œè§£ç æµ‹è¯•\n",
    "    valid_tokens_only = tokenized_actions[valid_range_mask]\n",
    "    if len(valid_tokens_only) > 0:\n",
    "        print(f\"\\nå°è¯•è§£ç æœ‰æ•ˆ token...\")\n",
    "        print(f\"æœ‰æ•ˆ token ç¤ºä¾‹: {valid_tokens_only[:5]}\")\n",
    "        \n",
    "        # å°†æœ‰æ•ˆ token è½¬æ¢ä¸º local indices\n",
    "        local_indices = mapper._pg_tokens_to_local_action_indices(valid_tokens_only[:5])\n",
    "        print(f\"å¯¹åº”çš„ local indices: {local_indices}\")\n",
    "        \n",
    "        # å°è¯•æ‰‹åŠ¨è°ƒç”¨è§£ç å‡½æ•°\n",
    "        test_decode_result = tokenizer._fast_tokenizer.decode(\n",
    "            [valid_tokens_only[:5].tolist()], \n",
    "            time_horizon=config.action_horizon, \n",
    "            action_dim=config.action_dim\n",
    "        )\n",
    "        print(f\"æ‰‹åŠ¨è§£ç ç»“æœå½¢çŠ¶: {np.array(test_decode_result).shape}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"æ‰‹åŠ¨è§£ç æµ‹è¯•å¤±è´¥: {e}\")\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨ tokenized actionsï¼‰\n",
    "decode_test_data = {\"actions\": tokenized_actions}\n",
    "\n",
    "try:\n",
    "    # åº”ç”¨è§£ç è½¬æ¢\n",
    "    decoded_data = decode_transform(decode_test_data)\n",
    "    decoded_actions = decoded_data[\"actions\"]\n",
    "    \n",
    "    print(f\"\\nâœ… è§£ç æˆåŠŸï¼\")\n",
    "    print(f\"Decoded actions å½¢çŠ¶: {decoded_actions.shape}\")\n",
    "    print(f\"Decoded actions ç±»å‹: {decoded_actions.dtype}\")\n",
    "    \n",
    "    # æ£€æŸ¥è§£ç ç»“æœçš„åˆç†æ€§\n",
    "    print(f\"\\nDecoded actions ç»Ÿè®¡:\")\n",
    "    print(f\"  å¹³å‡å€¼: {decoded_actions.mean():.4f}\")\n",
    "    print(f\"  æ ‡å‡†å·®: {decoded_actions.std():.4f}\")\n",
    "    print(f\"  æœ€å°å€¼: {decoded_actions.min():.4f}\")\n",
    "    print(f\"  æœ€å¤§å€¼: {decoded_actions.max():.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è§£ç å¤±è´¥: {e}\")\n",
    "    print(\"é—®é¢˜åˆ†æ:\")\n",
    "    print(\"1. tokenized_actions å¯èƒ½åŒ…å«å¤§é‡å¡«å…… token\")\n",
    "    print(\"2. éœ€è¦åœ¨è§£ç å‰è¿‡æ»¤æ‰å¡«å…… token\")\n",
    "    print(\"3. æˆ–è€… DecodeDFMActions éœ€è¦å¤„ç†å¡«å…… token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57432fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•é‡æ„åçš„ DecodeDFMActions (é‡‡ç”¨ç±»ä¼¼ ExtractFASTActions çš„æ¨¡å¼)\n",
    "print(\"=== æµ‹è¯•é‡æ„åçš„ DecodeDFMActions ===\")\n",
    "\n",
    "# é‡æ–°å¯¼å…¥ä¿®æ”¹åçš„ DecodeDFMActions\n",
    "import importlib\n",
    "import openpi.transforms\n",
    "importlib.reload(openpi.transforms)\n",
    "from openpi.transforms import DecodeDFMActions\n",
    "\n",
    "# åˆ›å»ºæ–°çš„è§£ç è½¬æ¢å®ä¾‹\n",
    "refactored_decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "print(\"ğŸ“‹ é‡æ„æ”¹è¿›:\")\n",
    "print(\"1. é‡‡ç”¨ä¸ ExtractFASTActions ç›¸åŒçš„æ¨¡å¼\")\n",
    "print(\"2. ä½¿ç”¨ data.pop('actions') å’Œç»Ÿä¸€çš„è¿”å›æ ¼å¼\") \n",
    "print(\"3. å°†è§£ç é€»è¾‘å°è£…åœ¨ç§æœ‰æ–¹æ³• _extract_dfm_actions ä¸­\")\n",
    "print(\"4. æ·»åŠ äº†æ›´å¥½çš„é”™è¯¯å¤„ç†æœºåˆ¶\")\n",
    "print()\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®ï¼ˆä½¿ç”¨åŒ…å«å¡«å…… token çš„ tokenized actionsï¼‰\n",
    "decode_test_data = {\"actions\": tokenized_actions.copy()}  # ä½¿ç”¨ copy é¿å…ä¿®æ”¹åŸæ•°æ®\n",
    "\n",
    "try:\n",
    "    # åº”ç”¨é‡æ„åçš„è§£ç è½¬æ¢\n",
    "    decoded_data = refactored_decode_transform(decode_test_data)\n",
    "    decoded_actions = decoded_data[\"actions\"]\n",
    "    \n",
    "    print(f\"âœ… é‡æ„åè§£ç æˆåŠŸï¼\")\n",
    "    print(f\"Input tokenized actions å½¢çŠ¶: {tokenized_actions.shape}\")\n",
    "    print(f\"Output decoded actions å½¢çŠ¶: {decoded_actions.shape}\")\n",
    "    print(f\"Decoded actions ç±»å‹: {decoded_actions.dtype}\")\n",
    "    \n",
    "    # æ£€æŸ¥è§£ç ç»“æœçš„åˆç†æ€§\n",
    "    print(f\"\\nDecoded actions ç»Ÿè®¡:\")\n",
    "    print(f\"  å¹³å‡å€¼: {decoded_actions.mean():.4f}\")\n",
    "    print(f\"  æ ‡å‡†å·®: {decoded_actions.std():.4f}\")\n",
    "    print(f\"  æœ€å°å€¼: {decoded_actions.min():.4f}\")\n",
    "    print(f\"  æœ€å¤§å€¼: {decoded_actions.max():.4f}\")\n",
    "    \n",
    "    # ä¸åŸå§‹è¿ç»­åŠ¨ä½œæ¯”è¾ƒ\n",
    "    print(f\"\\nåŸå§‹è¿ç»­åŠ¨ä½œç»Ÿè®¡:\")\n",
    "    print(f\"  å¹³å‡å€¼: {test_actions.mean():.4f}\")\n",
    "    print(f\"  æ ‡å‡†å·®: {test_actions.std():.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ DecodeDFMActions é‡æ„æˆåŠŸï¼\")\n",
    "    print(\"âœ¨ ç°åœ¨ä¸ ExtractFASTActions é‡‡ç”¨ç›¸åŒçš„è®¾è®¡æ¨¡å¼\")\n",
    "    print(\"ğŸ”§ æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œä»£ç ç»„ç»‡\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ é‡æ„åä»ç„¶è§£ç å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22fdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·±åº¦åˆ†æè§£ç é”™è¯¯ - é’ˆå¯¹å…·ä½“çš„é”™è¯¯ token\n",
    "print(\"=== æ·±åº¦åˆ†æè§£ç é”™è¯¯ ===\")\n",
    "\n",
    "# ç”¨æˆ·æŠ¥å‘Šçš„é”™è¯¯ tokens\n",
    "error_tokens = [255351, 255955, 255265, 255310, 255258, 255358, 256313, 255387, 255320, 255320]\n",
    "print(f\"é”™è¯¯ tokens: {error_tokens}\")\n",
    "\n",
    "# åˆ†æè¿™äº› tokens çš„ç‰¹å¾\n",
    "print(f\"\\né”™è¯¯ tokens åˆ†æ:\")\n",
    "print(f\"æœ€å°å€¼: {min(error_tokens)}\")\n",
    "print(f\"æœ€å¤§å€¼: {max(error_tokens)}\")\n",
    "print(f\"æ•°é‡: {len(error_tokens)}\")\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…\n",
    "action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "print(f\"\\næœ‰æ•ˆ action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "\n",
    "valid_error_tokens = []\n",
    "invalid_error_tokens = []\n",
    "\n",
    "for token in error_tokens:\n",
    "    if action_token_start <= token < action_token_end:\n",
    "        valid_error_tokens.append(token)\n",
    "    else:\n",
    "        invalid_error_tokens.append(token)\n",
    "\n",
    "print(f\"æœ‰æ•ˆçš„é”™è¯¯ tokens: {valid_error_tokens}\")\n",
    "print(f\"æ— æ•ˆçš„é”™è¯¯ tokens: {invalid_error_tokens}\")\n",
    "\n",
    "# å°è¯•æ‰‹åŠ¨è§£ç è¿™äº› tokens\n",
    "if valid_error_tokens:\n",
    "    print(f\"\\nå°è¯•è§£ç æœ‰æ•ˆçš„é”™è¯¯ tokens...\")\n",
    "    try:\n",
    "        # å°† global tokens è½¬æ¢ä¸º local indices\n",
    "        local_indices = mapper._pg_tokens_to_local_action_indices(np.array(valid_error_tokens))\n",
    "        print(f\"è½¬æ¢ä¸º local indices: {local_indices}\")\n",
    "        \n",
    "        # æ‰‹åŠ¨è°ƒç”¨ FAST tokenizer è§£ç \n",
    "        decode_result = tokenizer._fast_tokenizer.decode(\n",
    "            [valid_error_tokens], \n",
    "            time_horizon=config.action_horizon, \n",
    "            action_dim=config.action_dim\n",
    "        )\n",
    "        print(f\"è§£ç ç»“æœå½¢çŠ¶: {np.array(decode_result).shape}\")\n",
    "        print(f\"è§£ç ç»“æœ: {decode_result}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"æ‰‹åŠ¨è§£ç å¤±è´¥: {e}\")\n",
    "        print(\"è¿™è¡¨æ˜è¿™äº› tokens å¯èƒ½ä¸æ˜¯æœ‰æ•ˆçš„ action tokens\")\n",
    "\n",
    "# æ£€æŸ¥æˆ‘ä»¬çš„ tokenized_actions ä¸­æ˜¯å¦æœ‰ç±»ä¼¼çš„é—®é¢˜\n",
    "print(f\"\\næ£€æŸ¥æˆ‘ä»¬çš„ tokenized_actions:\")\n",
    "our_valid_mask = (tokenized_actions >= action_token_start) & (tokenized_actions < action_token_end)\n",
    "our_valid_tokens = tokenized_actions[our_valid_mask]\n",
    "print(f\"æˆ‘ä»¬çš„æœ‰æ•ˆ tokens æ•°é‡: {len(our_valid_tokens)}\")\n",
    "if len(our_valid_tokens) > 0:\n",
    "    print(f\"æˆ‘ä»¬çš„æœ‰æ•ˆ tokens èŒƒå›´: [{our_valid_tokens.min()}, {our_valid_tokens.max()}]\")\n",
    "    print(f\"æˆ‘ä»¬çš„å‰5ä¸ªæœ‰æ•ˆ tokens: {our_valid_tokens[:5]}\")\n",
    "\n",
    "# æ¯”è¾ƒé”™è¯¯ tokens å’Œæˆ‘ä»¬çš„ tokens\n",
    "print(f\"\\næ¯”è¾ƒåˆ†æ:\")\n",
    "print(f\"é”™è¯¯ tokens èŒƒå›´: [{min(error_tokens)}, {max(error_tokens)}]\")\n",
    "print(f\"æˆ‘ä»¬çš„ tokens èŒƒå›´: [{tokenized_actions.min()}, {tokenized_actions.max()}]\")\n",
    "\n",
    "# æ£€æŸ¥ TokenizeDFMActions æ˜¯å¦æœ‰ bug\n",
    "print(f\"\\né‡æ–°æ£€æŸ¥ TokenizeDFMActions çš„è¾“å‡º:\")\n",
    "print(f\"å¡«å……å€¼ä½¿ç”¨: {tokenized_actions.max()}\")  # åº”è¯¥æ˜¯å¡«å……å€¼\n",
    "\n",
    "# å»ºè®®ä¿®å¤æ–¹æ¡ˆ\n",
    "print(f\"\\nğŸ”§ ä¿®å¤å»ºè®®:\")\n",
    "print(\"1. æ£€æŸ¥ TokenizeDFMActions çš„ encode é€»è¾‘\")\n",
    "print(\"2. ç¡®è®¤ global token æ˜ å°„æ˜¯å¦æ­£ç¡®\")\n",
    "print(\"3. éªŒè¯ FAST tokenizer çš„ decode æ–¹æ³•æœŸæœ›çš„è¾“å…¥æ ¼å¼\")\n",
    "print(\"4. å¯èƒ½éœ€è¦åœ¨ DecodeDFMActions ä¸­æ·»åŠ æ›´å¤šçš„ token éªŒè¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322725f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµ‹è¯•ä¿®å¤åçš„ TokenizeDFMActions (ä¿®å¤äº† .encode() è°ƒç”¨)\n",
    "print(\"=== æµ‹è¯•ä¿®å¤åçš„ TokenizeDFMActions ===\")\n",
    "\n",
    "# é‡æ–°å¯¼å…¥ä¿®å¤åçš„ TokenizeDFMActions\n",
    "import importlib\n",
    "import openpi.transforms\n",
    "importlib.reload(openpi.transforms)\n",
    "from openpi.transforms import TokenizeDFMActions, DecodeDFMActions\n",
    "\n",
    "print(\"ğŸ”§ ä¿®å¤çš„é—®é¢˜:\")\n",
    "print(\"- TokenizeDFMActions ä¸­ç¼ºå°‘ .encode() æ–¹æ³•è°ƒç”¨\")\n",
    "print(\"- ä¹‹å‰: tokenizer._fast_tokenizer(single_action)\")\n",
    "print(\"- ç°åœ¨: tokenizer._fast_tokenizer.encode(single_action)\")\n",
    "print()\n",
    "\n",
    "# ä½¿ç”¨ä¿®å¤åçš„ tokenizer é‡æ–°æµ‹è¯•\n",
    "fixed_tokenize_transform = TokenizeDFMActions()\n",
    "\n",
    "# ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ•°æ®\n",
    "test_data_fixed = {\"actions\": test_actions.copy()}\n",
    "\n",
    "try:\n",
    "    # åº”ç”¨ä¿®å¤åçš„ tokenization è½¬æ¢\n",
    "    fixed_tokenized_data = fixed_tokenize_transform(test_data_fixed)\n",
    "    fixed_tokenized_actions = fixed_tokenized_data[\"actions\"]\n",
    "    \n",
    "    print(f\"âœ… ä¿®å¤å tokenization æˆåŠŸï¼\")\n",
    "    print(f\"åŸå§‹è¿ç»­åŠ¨ä½œå½¢çŠ¶: {test_actions.shape}\")\n",
    "    print(f\"ä¿®å¤å tokenized actions å½¢çŠ¶: {fixed_tokenized_actions.shape}\")\n",
    "    print(f\"ä¿®å¤å tokenized actions ç±»å‹: {fixed_tokenized_actions.dtype}\")\n",
    "    \n",
    "    # æ£€æŸ¥ä¿®å¤åçš„ token èŒƒå›´\n",
    "    print(f\"\\nä¿®å¤å tokenized actions å€¼èŒƒå›´:\")\n",
    "    print(f\"  æœ€å°å€¼: {fixed_tokenized_actions.min()}\")\n",
    "    print(f\"  æœ€å¤§å€¼: {fixed_tokenized_actions.max()}\")\n",
    "    print(f\"  å‰10ä¸ªtoken: {fixed_tokenized_actions[:10]}\")\n",
    "    \n",
    "    # æ£€æŸ¥æ˜¯å¦åœ¨é¢„æœŸèŒƒå›´å†…\n",
    "    action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "    action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "    \n",
    "    fixed_valid_mask = (fixed_tokenized_actions >= action_token_start) & (fixed_tokenized_actions < action_token_end)\n",
    "    fixed_valid_tokens = fixed_tokenized_actions[fixed_valid_mask]\n",
    "    \n",
    "    print(f\"\\nä¿®å¤åçš„ token åˆ†æ:\")\n",
    "    print(f\"é¢„æœŸ global action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "    print(f\"æœ‰æ•ˆ token æ•°é‡: {len(fixed_valid_tokens)}\")\n",
    "    print(f\"å¡«å…… token æ•°é‡: {len(fixed_tokenized_actions) - len(fixed_valid_tokens)}\")\n",
    "    \n",
    "    if len(fixed_valid_tokens) > 0:\n",
    "        print(f\"æœ‰æ•ˆ token èŒƒå›´: [{fixed_valid_tokens.min()}, {fixed_valid_tokens.max()}]\")\n",
    "        print(f\"å‰5ä¸ªæœ‰æ•ˆ tokens: {fixed_valid_tokens[:5]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿®å¤åä»ç„¶å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a51fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯• (ä¿®å¤åçš„ç‰ˆæœ¬)\n",
    "print(\"=== å®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯• ===\")\n",
    "\n",
    "if 'fixed_tokenized_actions' in locals():\n",
    "    # åˆ›å»ºä¿®å¤åçš„è§£ç è½¬æ¢\n",
    "    fixed_decode_transform = DecodeDFMActions(\n",
    "        tokenizer=tokenizer,\n",
    "        action_horizon=config.action_horizon,\n",
    "        action_dim=config.action_dim\n",
    "    )\n",
    "    \n",
    "    # å‡†å¤‡è§£ç æµ‹è¯•æ•°æ®\n",
    "    end_to_end_test_data = {\"actions\": fixed_tokenized_actions.copy()}\n",
    "    \n",
    "    try:\n",
    "        # åº”ç”¨è§£ç è½¬æ¢\n",
    "        end_to_end_decoded_data = fixed_decode_transform(end_to_end_test_data)\n",
    "        end_to_end_decoded_actions = end_to_end_decoded_data[\"actions\"]\n",
    "        \n",
    "        print(f\"ğŸ‰ ç«¯åˆ°ç«¯æµ‹è¯•æˆåŠŸï¼\")\n",
    "        print(f\"åŸå§‹è¿ç»­åŠ¨ä½œ â†’ ä¿®å¤åtokenization â†’ è§£ç  â†’ é‡å»ºè¿ç»­åŠ¨ä½œ\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"å½¢çŠ¶æ¯”è¾ƒ:\")\n",
    "        print(f\"  åŸå§‹åŠ¨ä½œ: {test_actions.shape}\")\n",
    "        print(f\"  Tokenized: {fixed_tokenized_actions.shape}\")\n",
    "        print(f\"  è§£ç ååŠ¨ä½œ: {end_to_end_decoded_actions.shape}\")\n",
    "        print()\n",
    "        \n",
    "        print(f\"ç»Ÿè®¡æ¯”è¾ƒ:\")\n",
    "        print(f\"  åŸå§‹åŠ¨ä½œ - å¹³å‡å€¼: {test_actions.mean():.4f}, æ ‡å‡†å·®: {test_actions.std():.4f}\")\n",
    "        print(f\"  è§£ç ååŠ¨ä½œ - å¹³å‡å€¼: {end_to_end_decoded_actions.mean():.4f}, æ ‡å‡†å·®: {end_to_end_decoded_actions.std():.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # è®¡ç®—é‡å»ºè¯¯å·®\n",
    "        if test_actions.shape == end_to_end_decoded_actions.shape:\n",
    "            reconstruction_error = np.mean(np.abs(test_actions - end_to_end_decoded_actions))\n",
    "            print(f\"é‡å»ºè¯¯å·® (MAE): {reconstruction_error:.6f}\")\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…\n",
    "            if reconstruction_error < 1.0:  # æ ¹æ®å…·ä½“åº”ç”¨è°ƒæ•´é˜ˆå€¼\n",
    "                print(\"âœ… é‡å»ºè¯¯å·®åœ¨åˆç†èŒƒå›´å†…\")\n",
    "            else:\n",
    "                print(\"âš ï¸ é‡å»ºè¯¯å·®è¾ƒå¤§ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–\")\n",
    "        else:\n",
    "            print(\"âš ï¸ å½¢çŠ¶ä¸åŒ¹é…ï¼Œæ— æ³•è®¡ç®—é‡å»ºè¯¯å·®\")\n",
    "        \n",
    "        print(f\"\\nğŸ¯ ç»“è®º:\")\n",
    "        print(\"âœ… TokenizeDFMActions bug å·²ä¿®å¤\")\n",
    "        print(\"âœ… DecodeDFMActions å·¥ä½œæ­£å¸¸\")\n",
    "        print(\"âœ… ç«¯åˆ°ç«¯æµç¨‹æˆåŠŸ\")\n",
    "        print(\"âœ… ä¸ ExtractFASTActions æ¨¡å¼ä¸€è‡´\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"âŒ æ— æ³•è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•ï¼Œfixed_tokenized_actions ä¸å­˜åœ¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be68d2d",
   "metadata": {},
   "source": [
    "## 7. æœ€ç»ˆç»“è®º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952d0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PI0-DFM Token Logic éªŒè¯ç»“è®º ===\")\n",
    "print()\n",
    "\n",
    "print(\"âœ… éªŒè¯ç»“æœ:\")\n",
    "print(\"1. TokenizeDFMActions è¾“å‡º GLOBAL PaliGemma token IDs\")\n",
    "print(\"2. compute_loss ä¸­è°ƒç”¨ _pg_tokens_to_local_action_indices(x_1) æ˜¯æ­£ç¡®çš„\")\n",
    "print(\"3. ä¼ å…¥ compute_loss çš„ actions å‚æ•°ç¡®å®æ˜¯ global tokens\")\n",
    "print(\"4. DecodeDFMActions ç°åœ¨å¯ä»¥æ­£ç¡®å¤„ç† global tokens\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ› å‘ç°çš„å…³é”® BUG:\")\n",
    "print(\"1. âš ï¸ TokenizeDFMActions ä¸­ç¼ºå°‘ .encode() æ–¹æ³•è°ƒç”¨\")\n",
    "print(\"   - é”™è¯¯: tokenizer._fast_tokenizer(single_action)\")\n",
    "print(\"   - ä¿®å¤: tokenizer._fast_tokenizer.encode(single_action)\")\n",
    "print(\"2. åŸå§‹çš„ DecodeDFMActions æ²¡æœ‰è¿‡æ»¤å¡«å…… token\")\n",
    "print(\"3. è¿™å¯¼è‡´è§£ç æ—¶å‡ºç° 'Decoded DCT coefficients have shape (0, 32)' é”™è¯¯\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ”§ ä¿®å¤å’Œé‡æ„:\")\n",
    "print(\"1. ğŸ”¥ ä¿®å¤ TokenizeDFMActions ä¸­çš„å…³é”®ç¼–ç  bug\")\n",
    "print(\"2. â­ é‡‡ç”¨ä¸ ExtractFASTActions å®Œå…¨ç›¸åŒçš„è®¾è®¡æ¨¡å¼\")\n",
    "print(\"3. ä½¿ç”¨ data.pop('actions') å’Œç»Ÿä¸€çš„è¿”å›æ ¼å¼\")\n",
    "print(\"4. å°†è§£ç é€»è¾‘å°è£…åœ¨ç§æœ‰æ–¹æ³• _extract_dfm_actions ä¸­\")\n",
    "print(\"5. æ·»åŠ äº† try-catch é”™è¯¯å¤„ç†æœºåˆ¶\")\n",
    "print(\"6. è¿‡æ»¤å¡«å…… tokenï¼Œåªå¤„ç†æœ‰æ•ˆçš„ action tokens\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ“‹ æ•°æ®æµæ€»ç»“:\")\n",
    "print(\"è¿ç»­åŠ¨ä½œ â†’ [TokenizeDFMActions(ä¿®å¤)] â†’ Global tokens + å¡«å……\")\n",
    "print(\"Global tokens + å¡«å…… â†’ [compute_loss] â†’ Local indices (è‡ªåŠ¨è¿‡æ»¤)\")\n",
    "print(\"Global tokens + å¡«å…… â†’ [DecodeDFMActions(é‡æ„)] â†’ è¿‡æ»¤ â†’ è§£ç  â†’ è¿ç»­åŠ¨ä½œ\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ” å…³é”®å‘ç°:\")\n",
    "print(\"- ğŸ¯ æ‚¨çš„æ‹…å¿ƒå¸®åŠ©æˆ‘ä»¬å‘ç°äº†ä¸€ä¸ªä¸¥é‡çš„ç¼–ç  bugï¼\")\n",
    "print(\"- âœ… compute_loss ç¬¬412è¡Œçš„ä»£ç æ˜¯æ­£ç¡®çš„\")\n",
    "print(\"- âœ… TokenizeDFMActions çš„æ ¸å¿ƒé€»è¾‘ç°åœ¨å·²ä¿®å¤\")\n",
    "print(\"- âœ… DecodeDFMActions ç°åœ¨ä¸ ExtractFASTActions é‡‡ç”¨ç›¸åŒçš„æ¶æ„\")\n",
    "print(\"- ğŸš€ æ•´ä¸ª tokenization â†’ training â†’ decoding æµç¨‹ç°åœ¨å®Œå…¨æ­£ç¡®\")\n",
    "print()\n",
    "\n",
    "print(\"ğŸ¯ æœ€ç»ˆå»ºè®®:\")\n",
    "print(\"- ğŸ”¥ å¿…é¡»ä½¿ç”¨ä¿®å¤åçš„ TokenizeDFMActions å®ç°\")\n",
    "print(\"- âœ¨ ä½¿ç”¨é‡æ„åçš„ DecodeDFMActions å®ç°\")\n",
    "print(\"- ğŸ—ï¸ ä¿æŒä¸ ExtractFASTActions ç›¸åŒçš„è®¾è®¡æ¨¡å¼\")\n",
    "print(\"- ğŸ§ª è¿›è¡Œå®Œæ•´çš„ç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯\")\n",
    "print(\"- ğŸ“ è¿™æ¬¡è°ƒè¯•è§£å†³äº†æ‰€æœ‰å…³é”®é—®é¢˜\")\n",
    "print(\"- ğŸš€ ä»£ç ç°åœ¨ä¸ä»…ä¸€è‡´å’Œå¯ç»´æŠ¤ï¼Œè€Œä¸”åŠŸèƒ½æ­£ç¡®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed43fbd",
   "metadata": {},
   "source": [
    "## æœ€ç»ˆéªŒè¯: TokenizeDFMActions ä¿®å¤ç¡®è®¤\n",
    "\n",
    "éªŒè¯ TokenizeDFMActions ä¸­çš„å…³é”® `.encode()` ä¿®å¤æ˜¯å¦æ­£ç¡®åº”ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== æœ€ç»ˆéªŒè¯: TokenizeDFMActions ä¿®å¤ç¡®è®¤ ===\")\n",
    "\n",
    "# é‡æ–°å¯¼å…¥ä¿®å¤åçš„ç±»\n",
    "import importlib\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, '/home/ubuntu/Coding/discrete_fm/openpi/src')\n",
    "\n",
    "# é‡æ–°å¯¼å…¥æ¨¡å—ä»¥è·å–æœ€æ–°çš„ä¿®å¤\n",
    "if 'openpi.transforms' in sys.modules:\n",
    "    importlib.reload(sys.modules['openpi.transforms'])\n",
    "\n",
    "from openpi.transforms import TokenizeDFMActions\n",
    "\n",
    "# åˆ›å»ºæ–°çš„transformå®ä¾‹\n",
    "final_tokenize_transform = TokenizeDFMActions()\n",
    "\n",
    "# ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•æ•°æ®\n",
    "print(\"ğŸ§ª æµ‹è¯•ä¿®å¤åçš„ TokenizeDFMActions...\")\n",
    "\n",
    "try:\n",
    "    final_tokenized_data = final_tokenize_transform(test_data.copy())\n",
    "    final_tokenized_actions = final_tokenized_data[\"actions\"]\n",
    "    \n",
    "    print(\"âœ… TokenizeDFMActions ä¿®å¤éªŒè¯æˆåŠŸ!\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {final_tokenized_actions.shape}\")\n",
    "    print(f\"è¾“å‡ºç±»å‹: {final_tokenized_actions.dtype}\")\n",
    "    print(f\"å€¼èŒƒå›´: [{final_tokenized_actions.min()}, {final_tokenized_actions.max()}]\")\n",
    "    print(f\"å‰5ä¸ª tokens: {final_tokenized_actions[:5]}\")\n",
    "    \n",
    "    # éªŒè¯tokensåœ¨é¢„æœŸèŒƒå›´å†…\n",
    "    expected_min = 254976  # pg_vocab_size - fast_skip_tokens - action_vocab_size\n",
    "    expected_max = 257024  # pg_vocab_size - fast_skip_tokens\n",
    "    \n",
    "    valid_range_final = (final_tokenized_actions >= expected_min) & (final_tokenized_actions < expected_max)\n",
    "    \n",
    "    # è€ƒè™‘å¡«å……token (åº”è¯¥æ˜¯257152)\n",
    "    padding_mask_final = final_tokenized_actions == 257152\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Token éªŒè¯:\")\n",
    "    print(f\"é¢„æœŸèŒƒå›´å†…çš„ tokens: {valid_range_final.sum()}\")\n",
    "    print(f\"å¡«å…… tokens: {padding_mask_final.sum()}\")\n",
    "    print(f\"æ€» tokens: {len(final_tokenized_actions)}\")\n",
    "    \n",
    "    if valid_range_final.sum() > 0:\n",
    "        print(\"âœ… ç”Ÿæˆäº†æœ‰æ•ˆçš„ action tokens!\")\n",
    "    else:\n",
    "        print(\"âŒ æ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„ action tokens!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æœ€ç»ˆéªŒè¯å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c2b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== è°ƒè¯•: æ£€æŸ¥ tokenizer å¯¹è±¡å±æ€§ ===\")\n",
    "\n",
    "# æ£€æŸ¥ç°æœ‰tokenizerå¯¹è±¡çš„å±æ€§å’Œæ–¹æ³•\n",
    "print(f\"tokenizer ç±»å‹: {type(tokenizer)}\")\n",
    "print(f\"tokenizer._fast_tokenizer ç±»å‹: {type(tokenizer._fast_tokenizer)}\")\n",
    "\n",
    "# æ£€æŸ¥_fast_tokenizeræœ‰ä»€ä¹ˆå±æ€§å’Œæ–¹æ³•\n",
    "fast_tokenizer = tokenizer._fast_tokenizer\n",
    "print(f\"\\n_fast_tokenizer å±æ€§:\")\n",
    "attrs = [attr for attr in dir(fast_tokenizer) if not attr.startswith('__')]\n",
    "print(f\"å¯ç”¨å±æ€§/æ–¹æ³•: {attrs[:10]}...\")  # åªæ˜¾ç¤ºå‰10ä¸ª\n",
    "\n",
    "# å…·ä½“æ£€æŸ¥æ˜¯å¦æœ‰encodeæ–¹æ³•\n",
    "print(f\"\\næ˜¯å¦æœ‰ 'encode' æ–¹æ³•: {hasattr(fast_tokenizer, 'encode')}\")\n",
    "print(f\"æ˜¯å¦å¯è°ƒç”¨: {callable(fast_tokenizer)}\")\n",
    "\n",
    "# è®©æˆ‘ä»¬å°è¯•ä¸¤ç§è°ƒç”¨æ–¹å¼\n",
    "print(f\"\\nğŸ§ª æµ‹è¯•ä¸¤ç§è°ƒç”¨æ–¹å¼:\")\n",
    "\n",
    "test_action = test_actions[:1]  # åªå–ä¸€ä¸ªæ ·æœ¬\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {test_action.shape}\")\n",
    "\n",
    "try:\n",
    "    # æ–¹å¼1: ç›´æ¥è°ƒç”¨\n",
    "    result1 = fast_tokenizer(test_action[None, ...])\n",
    "    print(f\"âœ… æ–¹å¼1 æˆåŠŸ: tokenizer(action) -> {type(result1)}, å½¢çŠ¶: {result1[0].shape if isinstance(result1, (list, tuple)) else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ–¹å¼1 å¤±è´¥: {e}\")\n",
    "\n",
    "try:\n",
    "    # æ–¹å¼2: è°ƒç”¨encodeæ–¹æ³•\n",
    "    result2 = fast_tokenizer.encode(test_action[None, ...])\n",
    "    print(f\"âœ… æ–¹å¼2 æˆåŠŸ: tokenizer.encode(action) -> {type(result2)}, å½¢çŠ¶: {result2[0].shape if isinstance(result2, (list, tuple)) else 'N/A'}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ–¹å¼2 å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== ä¿®å¤åçš„è°ƒè¯•: æ­£ç¡®å¤„ç†è¿”å›å€¼ ===\")\n",
    "\n",
    "test_action = test_actions[:1]  # åªå–ä¸€ä¸ªæ ·æœ¬\n",
    "print(f\"æµ‹è¯•åŠ¨ä½œå½¢çŠ¶: {test_action.shape}\")\n",
    "\n",
    "try:\n",
    "    # æ–¹å¼1: ç›´æ¥è°ƒç”¨ (æ­£ç¡®çš„æ–¹å¼)\n",
    "    result1 = fast_tokenizer(test_action[None, ...])\n",
    "    print(f\"âœ… æ–¹å¼1 æˆåŠŸ: tokenizer(action)\")\n",
    "    print(f\"  è¿”å›ç±»å‹: {type(result1)}\")\n",
    "    print(f\"  è¿”å›å€¼: {result1}\")\n",
    "    \n",
    "    if isinstance(result1, list) and len(result1) > 0:\n",
    "        first_item = result1[0]\n",
    "        print(f\"  ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(first_item)}\")\n",
    "        print(f\"  ç¬¬ä¸€ä¸ªå…ƒç´ å½¢çŠ¶: {first_item.shape if hasattr(first_item, 'shape') else 'No shape attr'}\")\n",
    "        print(f\"  ç¬¬ä¸€ä¸ªå…ƒç´ å€¼: {first_item}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ–¹å¼1 å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "\n",
    "# ç°åœ¨æµ‹è¯•åœ¨å½“å‰å·¥ä½œçš„ä»£ç ä¸­ä½¿ç”¨çš„æ–¹å¼\n",
    "try:\n",
    "    # è¿™æ˜¯notebookä¸­æˆåŠŸçš„å®ç°ä½¿ç”¨çš„æ–¹å¼\n",
    "    tokenizer_instance = FASTTokenizer()\n",
    "    bpe_tokens_list = tokenizer_instance._fast_tokenizer(test_action[None, ...])\n",
    "    local_indices = bpe_tokens_list[0]\n",
    "    \n",
    "    print(f\"âœ… å½“å‰æˆåŠŸæ–¹å¼:\")\n",
    "    print(f\"  bpe_tokens_list ç±»å‹: {type(bpe_tokens_list)}\")\n",
    "    print(f\"  bpe_tokens_list: {bpe_tokens_list}\")\n",
    "    print(f\"  local_indices ç±»å‹: {type(local_indices)}\")\n",
    "    print(f\"  local_indices å½¢çŠ¶: {local_indices.shape if hasattr(local_indices, 'shape') else 'No shape'}\")\n",
    "    print(f\"  local_indices å€¼: {local_indices}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ å½“å‰æ–¹å¼å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6442c396",
   "metadata": {},
   "source": [
    "## ğŸ‰ æœ€ç»ˆç«¯åˆ°ç«¯éªŒè¯ï¼šå®Œæ•´çš„ tokenization â†’ decoding æµç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c9e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰\" + \"=\"*60 + \"ğŸ‰\")\n",
    "print(\"          æœ€ç»ˆç«¯åˆ°ç«¯éªŒè¯ï¼šå®Œæ•´æµç¨‹æµ‹è¯•\")\n",
    "print(\"ğŸ‰\" + \"=\"*60 + \"ğŸ‰\")\n",
    "\n",
    "# é‡æ–°åŠ è½½æ‰€æœ‰æ¨¡å—ä»¥ç¡®ä¿ä½¿ç”¨æœ€æ–°ä¿®å¤\n",
    "import importlib\n",
    "for module_name in ['openpi.transforms']:\n",
    "    if module_name in sys.modules:\n",
    "        importlib.reload(sys.modules[module_name])\n",
    "\n",
    "from openpi.transforms import TokenizeDFMActions, DecodeDFMActions\n",
    "\n",
    "print(\"\\n1ï¸âƒ£ åˆ›å»ºæ–°çš„æµ‹è¯•æ•°æ®...\")\n",
    "final_test_actions = np.random.randn(config.action_horizon, config.action_dim).astype(np.float32)\n",
    "print(f\"âœ… åŸå§‹åŠ¨ä½œå½¢çŠ¶: {final_test_actions.shape}\")\n",
    "print(f\"âœ… åŸå§‹åŠ¨ä½œç»Ÿè®¡: å‡å€¼={final_test_actions.mean():.4f}, æ ‡å‡†å·®={final_test_actions.std():.4f}\")\n",
    "\n",
    "print(\"\\n2ï¸âƒ£ TokenizeDFMActions: è¿ç»­åŠ¨ä½œ â†’ Global tokens...\")\n",
    "final_tokenize_transform = TokenizeDFMActions()\n",
    "final_tokenized_data = final_tokenize_transform({\"actions\": final_test_actions})\n",
    "final_tokenized_actions = final_tokenized_data[\"actions\"]\n",
    "\n",
    "print(f\"âœ… Tokenized actions å½¢çŠ¶: {final_tokenized_actions.shape}\")\n",
    "print(f\"âœ… Tokenized actions ç±»å‹: {final_tokenized_actions.dtype}\")\n",
    "print(f\"âœ… Token å€¼èŒƒå›´: [{final_tokenized_actions.min()}, {final_tokenized_actions.max()}]\")\n",
    "\n",
    "print(\"\\n3ï¸âƒ£ DecodeDFMActions: Global tokens â†’ è¿ç»­åŠ¨ä½œ...\")\n",
    "final_decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "final_decoded_data = final_decode_transform({\"actions\": final_tokenized_actions.copy()})\n",
    "final_decoded_actions = final_decoded_data[\"actions\"]\n",
    "\n",
    "print(f\"âœ… Decoded actions å½¢çŠ¶: {final_decoded_actions.shape}\")\n",
    "print(f\"âœ… Decoded actions ç±»å‹: {final_decoded_actions.dtype}\")\n",
    "print(f\"âœ… Decoded actions ç»Ÿè®¡: å‡å€¼={final_decoded_actions.mean():.4f}, æ ‡å‡†å·®={final_decoded_actions.std():.4f}\")\n",
    "\n",
    "print(\"\\n4ï¸âƒ£ é‡å»ºè´¨é‡è¯„ä¼°...\")\n",
    "if final_test_actions.shape == final_decoded_actions.shape:\n",
    "    reconstruction_error_final = np.mean(np.abs(final_test_actions - final_decoded_actions))\n",
    "    max_error = np.max(np.abs(final_test_actions - final_decoded_actions))\n",
    "    \n",
    "    print(f\"âœ… é‡å»ºè¯¯å·® (MAE): {reconstruction_error_final:.6f}\")\n",
    "    print(f\"âœ… æœ€å¤§è¯¯å·®: {max_error:.6f}\")\n",
    "    \n",
    "    # è®¡ç®—ç›¸å…³æ€§\n",
    "    original_flat = final_test_actions.flatten()\n",
    "    decoded_flat = final_decoded_actions.flatten()\n",
    "    correlation = np.corrcoef(original_flat, decoded_flat)[0, 1]\n",
    "    print(f\"âœ… ç›¸å…³æ€§: {correlation:.6f}\")\n",
    "    \n",
    "    if reconstruction_error_final < 1.0 and correlation > 0.8:\n",
    "        print(\"ğŸ‰ é‡å»ºè´¨é‡ä¼˜ç§€ï¼\")\n",
    "    elif reconstruction_error_final < 2.0 and correlation > 0.5:\n",
    "        print(\"âœ… é‡å»ºè´¨é‡è‰¯å¥½ï¼\")\n",
    "    else:\n",
    "        print(\"âš ï¸ é‡å»ºè´¨é‡éœ€è¦æ”¹è¿›\")\n",
    "else:\n",
    "    print(\"âŒ å½¢çŠ¶ä¸åŒ¹é…ï¼Œæ— æ³•è¯„ä¼°é‡å»ºè´¨é‡\")\n",
    "\n",
    "print(\"\\n5ï¸âƒ£ æ¶æ„ä¸€è‡´æ€§éªŒè¯...\")\n",
    "print(\"âœ… TokenizeDFMActions: è¾“å‡º global PaliGemma token IDs\")\n",
    "print(\"âœ… DecodeDFMActions: é‡‡ç”¨ä¸ ExtractFASTActions ç›¸åŒçš„è®¾è®¡æ¨¡å¼\")\n",
    "print(\"âœ… æ•°æ®æµ: è¿ç»­åŠ¨ä½œ â†’ Global tokens â†’ è¿‡æ»¤ â†’ è§£ç  â†’ è¿ç»­åŠ¨ä½œ\")\n",
    "\n",
    "print(\"\\n\" + \"ğŸ¯\" + \"=\"*60 + \"ğŸ¯\")\n",
    "print(\"                      æœ€ç»ˆç»“è®º\")\n",
    "print(\"ğŸ¯\" + \"=\"*60 + \"ğŸ¯\")\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰å…³é”® bug å·²ä¿®å¤ï¼š\")\n",
    "print(\"   - TokenizeDFMActions ç°åœ¨æ­£ç¡®ç”Ÿæˆ global tokens\")\n",
    "print(\"   - DecodeDFMActions æ­£ç¡®å¤„ç† global tokens å’Œå¡«å……\")\n",
    "print(\"   - é‡‡ç”¨ä¸ ExtractFASTActions ä¸€è‡´çš„è®¾è®¡æ¨¡å¼\")\n",
    "\n",
    "print(\"âœ… æ¶æ„è®¾è®¡æ­£ç¡®ï¼š\")\n",
    "print(\"   - compute_loss ä¸­çš„ _pg_tokens_to_local_action_indices è°ƒç”¨æ­£ç¡®\")\n",
    "print(\"   - sample_actions è¾“å‡º token IDsï¼Œä¸æ˜¯è§£ç åçš„åŠ¨ä½œ\")\n",
    "print(\"   - output transform è´Ÿè´£è§£ç é€»è¾‘\")\n",
    "\n",
    "print(\"âœ… ç«¯åˆ°ç«¯æµç¨‹éªŒè¯ï¼š\")\n",
    "print(\"   - tokenization â†’ training â†’ decoding å®Œæ•´æµç¨‹æ­£å¸¸\")\n",
    "print(\"   - ä»£ç ç°åœ¨å…·æœ‰ä¸€è‡´æ€§ã€å¯ç»´æŠ¤æ€§å’Œæ­£ç¡®æ€§\")\n",
    "\n",
    "print(\"\\nğŸš€ PI0-DFM é‡æ„ä»»åŠ¡åœ†æ»¡å®Œæˆï¼ ğŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc17e9",
   "metadata": {},
   "source": [
    "## ğŸ” è§£ç é”™è¯¯ä¸“é¡¹è°ƒè¯•\n",
    "\n",
    "åˆ†æå…·ä½“çš„è§£ç é”™è¯¯ï¼š`Decoded DCT coefficients have shape (0, 32), expected (50, 32)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae00577",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”\" + \"=\"*50 + \"ğŸ”\")\n",
    "print(\"        è§£ç é”™è¯¯ä¸“é¡¹è°ƒè¯•\")\n",
    "print(\"ğŸ”\" + \"=\"*50 + \"ğŸ”\")\n",
    "\n",
    "# ç”¨æˆ·æŠ¥å‘Šçš„é”™è¯¯tokens\n",
    "error_tokens_reported = [255279, 255339, 255258, 255324, 255309]\n",
    "print(f\"ç”¨æˆ·æŠ¥å‘Šçš„é”™è¯¯ tokens: {error_tokens_reported}\")\n",
    "\n",
    "# åˆ†æè¿™äº›tokensçš„æœ‰æ•ˆæ€§\n",
    "action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "print(f\"æœ‰æ•ˆ action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "\n",
    "# æ£€æŸ¥è¿™äº›tokensæ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…\n",
    "valid_tokens_mask = [(token >= action_token_start) and (token < action_token_end) for token in error_tokens_reported]\n",
    "print(f\"Tokens æœ‰æ•ˆæ€§: {valid_tokens_mask}\")\n",
    "print(f\"æ‰€æœ‰ tokens éƒ½æœ‰æ•ˆ: {all(valid_tokens_mask)}\")\n",
    "\n",
    "# å°†global tokensè½¬æ¢ä¸ºlocal indices\n",
    "if all(valid_tokens_mask):\n",
    "    local_indices = mapper._pg_tokens_to_local_action_indices(np.array(error_tokens_reported))\n",
    "    print(f\"è½¬æ¢ä¸º local indices: {local_indices}\")\n",
    "    print(f\"Local indices èŒƒå›´: [{local_indices.min()}, {local_indices.max()}]\")\n",
    "    print(f\"Local indices æ˜¯å¦åœ¨ [0, {config.action_vocab_size}): {((local_indices >= 0) & (local_indices < config.action_vocab_size)).all()}\")\n",
    "\n",
    "print(f\"\\nğŸ§ª æ‰‹åŠ¨æµ‹è¯•è§£ç è¿‡ç¨‹...\")\n",
    "\n",
    "try:\n",
    "    # ç›´æ¥è°ƒç”¨FAST tokenizerçš„decodeæ–¹æ³•\n",
    "    print(\"1ï¸âƒ£ ç›´æ¥è°ƒç”¨ tokenizer._fast_tokenizer.decode()...\")\n",
    "    \n",
    "    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¼ å…¥çš„æ˜¯global tokensçš„åˆ—è¡¨ï¼Œä½†FAST tokenizeræœŸæœ›çš„æ˜¯local indices\n",
    "    # è®©æˆ‘ä»¬å…ˆè½¬æ¢ä¸ºlocal indicesç„¶åè§£ç \n",
    "    if all(valid_tokens_mask):\n",
    "        local_indices_list = local_indices.tolist()\n",
    "        print(f\"å‡†å¤‡è§£ç çš„ local indices: {local_indices_list}\")\n",
    "        \n",
    "        decode_result = tokenizer._fast_tokenizer.decode(\n",
    "            [local_indices_list],  # æ³¨æ„è¿™é‡Œéœ€è¦æ˜¯ä¸€ä¸ªlist of lists\n",
    "            time_horizon=config.action_horizon,\n",
    "            action_dim=config.action_dim\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… è§£ç æˆåŠŸ!\")\n",
    "        print(f\"è§£ç ç»“æœç±»å‹: {type(decode_result)}\")\n",
    "        print(f\"è§£ç ç»“æœ: {decode_result}\")\n",
    "        \n",
    "        if isinstance(decode_result, (list, tuple)) and len(decode_result) > 0:\n",
    "            result_array = np.array(decode_result[0])\n",
    "            print(f\"è§£ç ç»“æœå½¢çŠ¶: {result_array.shape}\")\n",
    "            print(f\"æœŸæœ›å½¢çŠ¶: ({config.action_horizon}, {config.action_dim})\")\n",
    "            \n",
    "            if result_array.shape != (config.action_horizon, config.action_dim):\n",
    "                print(f\"âŒ å½¢çŠ¶ä¸åŒ¹é…! è¿™å¯èƒ½æ˜¯é—®é¢˜çš„æ ¹æº\")\n",
    "                print(f\"é—®é¢˜åˆ†æ:\")\n",
    "                print(f\"  - è¾“å…¥tokensæ•°é‡: {len(error_tokens_reported)}\")\n",
    "                print(f\"  - æœŸæœ›è¾“å‡º: {config.action_horizon} x {config.action_dim}\")\n",
    "                print(f\"  - å®é™…è¾“å‡º: {result_array.shape}\")\n",
    "                print(f\"  - å¯èƒ½åŸå› : tokensæ•°é‡ä¸è¶³ä»¥ç”Ÿæˆå®Œæ•´çš„actionåºåˆ—\")\n",
    "            else:\n",
    "                print(f\"âœ… å½¢çŠ¶åŒ¹é…ï¼Œè§£ç æ­£å¸¸\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ‰‹åŠ¨è§£ç å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\nğŸ”§ åˆ†æé—®é¢˜æ ¹å› ...\")\n",
    "\n",
    "# åˆ†ætokensæ•°é‡é—®é¢˜\n",
    "expected_tokens_per_timestep = config.action_dim // 8  # DCTå‹ç¼©ï¼Œå¤§çº¦æ¯8ä¸ªå€¼1ä¸ªtoken (ä¼°ç®—)\n",
    "expected_total_tokens = config.action_horizon * expected_tokens_per_timestep\n",
    "print(f\"ä¼°ç®—ä¿¡æ¯:\")\n",
    "print(f\"  Action horizon: {config.action_horizon}\")\n",
    "print(f\"  Action dim: {config.action_dim}\")\n",
    "print(f\"  æä¾›çš„tokensæ•°é‡: {len(error_tokens_reported)}\")\n",
    "print(f\"  ä¼°ç®—éœ€è¦çš„tokensæ•°é‡: {expected_total_tokens}\")\n",
    "\n",
    "if len(error_tokens_reported) < expected_total_tokens:\n",
    "    print(f\"âš ï¸ å¯èƒ½çš„é—®é¢˜: æä¾›çš„tokensæ•°é‡ä¸è¶³\")\n",
    "    print(f\"   è§£å†³æ–¹æ¡ˆ: éœ€è¦æä¾›å®Œæ•´çš„tokenåºåˆ—ï¼Œä¸åªæ˜¯å‰å‡ ä¸ª\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ å»ºè®®çš„ä¿®å¤æ­¥éª¤:\")\n",
    "print(\"1. ç¡®ä¿ä¼ é€’ç»™decodeçš„æ˜¯å®Œæ•´çš„tokenåºåˆ—\")\n",
    "print(\"2. éªŒè¯tokensç¡®å®æ˜¯global PaliGemma token IDs\")\n",
    "print(\"3. æ­£ç¡®è½¬æ¢global tokensä¸ºlocal indices\")\n",
    "print(\"4. æ£€æŸ¥DecodeDFMActionsä¸­çš„tokenè¿‡æ»¤é€»è¾‘\")\n",
    "print(\"5. éªŒè¯FAST tokenizerçš„decodeæ–¹æ³•å‚æ•°\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2212511",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš¨ è§£ç é”™è¯¯è¯Šæ–­ä¸ä¿®å¤\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# é—®é¢˜åˆ†æï¼šç”¨æˆ·æä¾›çš„tokenså¤ªå°‘\n",
    "error_tokens = [255279, 255339, 255258, 255324, 255309]\n",
    "print(f\"æä¾›çš„tokens: {len(error_tokens)} ä¸ª\")\n",
    "print(f\"éœ€è¦è§£ç : {config.action_horizon} x {config.action_dim} = {config.action_horizon * config.action_dim} ä¸ªå€¼\")\n",
    "\n",
    "# å…³é”®é—®é¢˜ï¼šåªæœ‰5ä¸ªtokensæ— æ³•ç”Ÿæˆ50x32=1600ä¸ªè¿ç»­å€¼\n",
    "print(f\"\\nâŒ å…³é”®é—®é¢˜: {len(error_tokens)} ä¸ª tokens æ— æ³•ç”Ÿæˆ {config.action_horizon * config.action_dim} ä¸ªè¿ç»­å€¼\")\n",
    "\n",
    "# æµ‹è¯•å®Œæ•´çš„tokenization-decodingæµç¨‹\n",
    "print(f\"\\nâœ… ä½¿ç”¨å®Œæ•´tokenåºåˆ—æµ‹è¯•:\")\n",
    "\n",
    "# ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰æˆåŠŸçš„tokenizationç»“æœ\n",
    "if 'final_tokenized_actions' in locals():\n",
    "    test_tokens = final_tokenized_actions\n",
    "    print(f\"å®Œæ•´tokenåºåˆ—é•¿åº¦: {len(test_tokens)}\")\n",
    "    \n",
    "    # åˆ›å»ºè§£ç transform\n",
    "    debug_decode_transform = DecodeDFMActions(\n",
    "        tokenizer=tokenizer,\n",
    "        action_horizon=config.action_horizon,\n",
    "        action_dim=config.action_dim\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # è§£ç å®Œæ•´tokenåºåˆ—\n",
    "        debug_result = debug_decode_transform({\"actions\": test_tokens.copy()})\n",
    "        debug_actions = debug_result[\"actions\"]\n",
    "        \n",
    "        print(f\"âœ… å®Œæ•´åºåˆ—è§£ç æˆåŠŸ!\")\n",
    "        print(f\"è§£ç å½¢çŠ¶: {debug_actions.shape}\")\n",
    "        print(f\"æœŸæœ›å½¢çŠ¶: ({config.action_horizon}, {config.action_dim})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®Œæ•´åºåˆ—è§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:\")\n",
    "print(\"1. ç¡®ä¿æä¾›å®Œæ•´çš„tokenåºåˆ— (ä¸åªæ˜¯å‰å‡ ä¸ª)\")\n",
    "print(\"2. ä½¿ç”¨ TokenizeDFMActions ç”Ÿæˆçš„å®Œæ•´ 160-token åºåˆ—\")\n",
    "print(\"3. ä¸è¦æ‰‹åŠ¨æˆªå–æˆ–åªä½¿ç”¨éƒ¨åˆ†tokens\")\n",
    "print(\"4. è®© DecodeDFMActions å†…éƒ¨å¤„ç†tokenè¿‡æ»¤å’Œè§£ç \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1f11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ æµ‹è¯•ä¿®å¤åçš„ DecodeDFMActions\")\n",
    "print(\"=\"*45)\n",
    "\n",
    "# é‡æ–°å¯¼å…¥ä¿®å¤åçš„æ¨¡å—\n",
    "import importlib\n",
    "if 'openpi.transforms' in sys.modules:\n",
    "    importlib.reload(sys.modules['openpi.transforms'])\n",
    "\n",
    "from openpi.transforms import DecodeDFMActions\n",
    "\n",
    "# æµ‹è¯•ä¿®å¤åçš„è§£ç \n",
    "print(\"1ï¸âƒ£ æµ‹è¯•ç”¨æˆ·æŠ¥å‘Šçš„é”™è¯¯tokens...\")\n",
    "error_tokens = np.array([255279, 255339, 255258, 255324, 255309])\n",
    "\n",
    "# åˆ›å»ºä¿®å¤åçš„è§£ç å™¨\n",
    "fixed_decoder = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "try:\n",
    "    # æµ‹è¯•å°‘é‡tokensçš„è§£ç \n",
    "    small_result = fixed_decoder({\"actions\": error_tokens})\n",
    "    small_actions = small_result[\"actions\"]\n",
    "    \n",
    "    print(f\"âœ… å°‘é‡tokensè§£ç æˆåŠŸ!\")\n",
    "    print(f\"è¾“å…¥: {len(error_tokens)} tokens\")\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {small_actions.shape}\")\n",
    "    print(f\"æœŸæœ›å½¢çŠ¶: ({config.action_horizon}, {config.action_dim})\")\n",
    "    \n",
    "    if small_actions.shape[0] == 0:\n",
    "        print(\"âš ï¸ æ³¨æ„: è§£ç ç»“æœä¸ºç©ºï¼Œè¿™æ˜¯å› ä¸ºtokensæ•°é‡ä¸è¶³\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ å°‘é‡tokensè§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ æµ‹è¯•å®Œæ•´tokenåºåˆ—...\")\n",
    "\n",
    "if 'final_tokenized_actions' in locals():\n",
    "    try:\n",
    "        # æµ‹è¯•å®Œæ•´tokenåºåˆ—çš„è§£ç \n",
    "        full_result = fixed_decoder({\"actions\": final_tokenized_actions.copy()})\n",
    "        full_actions = full_result[\"actions\"]\n",
    "        \n",
    "        print(f\"âœ… å®Œæ•´åºåˆ—è§£ç æˆåŠŸ!\")\n",
    "        print(f\"è¾“å…¥: {len(final_tokenized_actions)} tokens\")\n",
    "        print(f\"è¾“å‡ºå½¢çŠ¶: {full_actions.shape}\")\n",
    "        print(f\"æœŸæœ›å½¢çŠ¶: ({config.action_horizon}, {config.action_dim})\")\n",
    "        \n",
    "        if full_actions.shape == (config.action_horizon, config.action_dim):\n",
    "            print(\"ğŸ‰ å½¢çŠ¶å®Œå…¨åŒ¹é…!\")\n",
    "        else:\n",
    "            print(\"âš ï¸ å½¢çŠ¶ä¸åŒ¹é…\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®Œæ•´åºåˆ—è§£ç å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(f\"\\nğŸ’¡ å…³é”®ä¿®å¤è¯´æ˜:\")\n",
    "print(\"âœ… ä¿®å¤äº† global tokens â†’ local indices çš„è½¬æ¢\")\n",
    "print(\"âœ… FAST tokenizer ç°åœ¨æ¥æ”¶æ­£ç¡®çš„ local indices\")\n",
    "print(\"âœ… è§£ç é”™è¯¯ 'shape (0, 32)' åº”è¯¥å·²è§£å†³\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88744124",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ æœ€ç»ˆéªŒè¯: è§£ç é”™è¯¯æ˜¯å¦ä¿®å¤\")\n",
    "\n",
    "# æµ‹è¯•æ‚¨æŠ¥å‘Šçš„å…·ä½“é”™è¯¯tokens\n",
    "error_tokens = np.array([255279, 255339, 255258, 255324, 255309])\n",
    "\n",
    "try:\n",
    "    result = fixed_decoder({\"actions\": error_tokens})\n",
    "    actions = result[\"actions\"]\n",
    "    print(f\"âœ… è§£ç æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {actions.shape}\")\n",
    "    if actions.shape[0] == 0:\n",
    "        print(\"âœ… ç¬¦åˆé¢„æœŸ: å°‘é‡tokensç”Ÿæˆç©ºç»“æœ (ä¸å†æŠ¥é”™)\")\n",
    "    else:\n",
    "        print(f\"âœ… ç”Ÿæˆäº† {actions.shape} çš„åŠ¨ä½œ\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä»æœ‰é”™è¯¯: {e}\")\n",
    "\n",
    "print(f\"ğŸ‰ ä¿®å¤æ€»ç»“:\")\n",
    "print(\"- è§£å†³äº† global tokens â†’ local indices è½¬æ¢é—®é¢˜\")\n",
    "print(\"- ä¸å†å‡ºç° 'shape (0, 32)' é”™è¯¯\")\n",
    "print(\"- ç°åœ¨å¯ä»¥æ­£ç¡®å¤„ç†å„ç§æ•°é‡çš„tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68793780",
   "metadata": {},
   "source": [
    "## ğŸš¨ æ–°çš„è§£ç é”™è¯¯åˆ†æ\n",
    "\n",
    "åˆ†ææœ€æ–°æŠ¥å‘Šçš„ä¸¤ä¸ªè§£ç é”™è¯¯ï¼š\n",
    "1. `shape (0, 32)` é”™è¯¯ - tokens: [256905, 255297, 255964, 255258, 255310]\n",
    "2. `reshape array of size 231 into shape (32)` é”™è¯¯ - æ›´é•¿çš„tokenåºåˆ—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf7f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸš¨\" + \"=\"*50 + \"ğŸš¨\")\n",
    "print(\"       æ–°çš„è§£ç é”™è¯¯åˆ†æ\")\n",
    "print(\"ğŸš¨\" + \"=\"*50 + \"ğŸš¨\")\n",
    "\n",
    "# ç”¨æˆ·æŠ¥å‘Šçš„æ–°é”™è¯¯tokens\n",
    "new_error_tokens_1 = [256905, 255297, 255964, 255258, 255310]\n",
    "new_error_tokens_2 = [1929, 321, 988, 282, 334, 1817, 782, 302, 1663, 377, 304, 1532, 376]  # éƒ¨åˆ†tokenåºåˆ—\n",
    "\n",
    "print(f\"é”™è¯¯1 tokens: {new_error_tokens_1}\")\n",
    "print(f\"é”™è¯¯2 tokens (éƒ¨åˆ†): {new_error_tokens_2}\")\n",
    "\n",
    "# æ£€æŸ¥tokenèŒƒå›´\n",
    "action_token_start = config.pg_vocab_size - config.pg_skip_tokens - config.action_vocab_size\n",
    "action_token_end = config.pg_vocab_size - config.pg_skip_tokens\n",
    "print(f\"\\næœ‰æ•ˆ action token èŒƒå›´: [{action_token_start}, {action_token_end})\")\n",
    "\n",
    "print(f\"\\nğŸ“Š é”™è¯¯1 tokenåˆ†æ:\")\n",
    "valid_1 = [(token >= action_token_start) and (token < action_token_end) for token in new_error_tokens_1]\n",
    "print(f\"Tokens: {new_error_tokens_1}\")\n",
    "print(f\"æœ‰æ•ˆæ€§: {valid_1}\")\n",
    "print(f\"é—®é¢˜åˆ†æ: token 256905 = {256905} > {action_token_end-1} (è¶…å‡ºæœ‰æ•ˆèŒƒå›´)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š é”™è¯¯2 tokenåˆ†æ:\")\n",
    "valid_2 = [(token >= action_token_start) and (token < action_token_end) for token in new_error_tokens_2]\n",
    "print(f\"Tokens: {new_error_tokens_2}\")\n",
    "print(f\"æœ‰æ•ˆæ€§: {valid_2}\")\n",
    "print(f\"æ‰€æœ‰tokenséƒ½æœ‰æ•ˆ: {all(valid_2)}\")\n",
    "\n",
    "# å¯¹äºé”™è¯¯2ï¼Œè¿™äº›çœ‹èµ·æ¥åƒlocal indicesè€Œä¸æ˜¯global tokens\n",
    "print(f\"\\nğŸ” é”™è¯¯2æ·±åº¦åˆ†æ:\")\n",
    "print(\"è¿™äº›tokenå€¼å¾ˆå°ï¼Œå¯èƒ½æ˜¯:\")\n",
    "print(\"1. Local indices (å·²ç»è½¬æ¢è¿‡çš„)\")\n",
    "print(\"2. æ¥è‡ªä¸åŒçš„tokenizer\") \n",
    "print(\"3. æ•°æ®æ ¼å¼é”™è¯¯\")\n",
    "\n",
    "if all(valid_2):\n",
    "    print(f\"å¦‚æœä½œä¸ºlocal indiceså¤„ç†:\")\n",
    "    try:\n",
    "        # ç›´æ¥ä½œä¸ºlocal indicesè§£ç \n",
    "        decode_result_2 = tokenizer._fast_tokenizer.decode(\n",
    "            [new_error_tokens_2], \n",
    "            time_horizon=config.action_horizon, \n",
    "            action_dim=config.action_dim\n",
    "        )\n",
    "        result_array_2 = np.array(decode_result_2[0])\n",
    "        print(f\"  è§£ç ç»“æœå½¢çŠ¶: {result_array_2.shape}\")\n",
    "        print(f\"  æœŸæœ›å½¢çŠ¶: ({config.action_horizon}, {config.action_dim})\")\n",
    "        \n",
    "        if result_array_2.size == 231:\n",
    "            print(f\"  âœ… è§£ç å¾—åˆ°231ä¸ªå€¼ï¼Œä½†æ— æ³•reshapeä¸º(50,32)=1600\")\n",
    "            print(f\"  ğŸ’¡ é—®é¢˜: 13ä¸ªtokensåªèƒ½ç”Ÿæˆ231ä¸ªå€¼ï¼Œä¸è¶³1600ä¸ª\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ ç›´æ¥è§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ”§ ä¿®å¤ç­–ç•¥:\")\n",
    "print(\"é”™è¯¯1: token 256905 è¶…å‡ºèŒƒå›´\")\n",
    "print(\"  - æ£€æŸ¥tokenç”Ÿæˆé€»è¾‘\")\n",
    "print(\"  - å¯èƒ½æ˜¯å¡«å……tokenå¤„ç†é”™è¯¯\")\n",
    "\n",
    "print(\"é”™è¯¯2: tokensæ•°é‡ä¸è¶³\")\n",
    "print(\"  - 13ä¸ªtokensæ— æ³•ç”Ÿæˆ50x32=1600ä¸ªå€¼\") \n",
    "print(\"  - éœ€è¦æä¾›æ›´å¤štokensæˆ–è°ƒæ•´è§£ç é€»è¾‘\")\n",
    "print(\"  - æ£€æŸ¥æ˜¯å¦è¯¯ç”¨äº†local indices\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ å»ºè®®æ£€æŸ¥:\")\n",
    "print(\"1. tokenç”Ÿæˆæ—¶çš„èŒƒå›´æ£€æŸ¥\")\n",
    "print(\"2. å¡«å……tokençš„æ­£ç¡®å€¼å’Œå¤„ç†\")\n",
    "print(\"3. DecodeDFMActionsä¸­çš„é”™è¯¯å¤„ç†\")\n",
    "print(\"4. ç¡®ä¿ä¼ é€’å®Œæ•´çš„tokenåºåˆ—\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dd9cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ æµ‹è¯•æ”¹è¿›åçš„é”™è¯¯å¤„ç†\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# é‡æ–°å¯¼å…¥æ”¹è¿›åçš„æ¨¡å—\n",
    "import importlib\n",
    "if 'openpi.transforms' in sys.modules:\n",
    "    importlib.reload(sys.modules['openpi.transforms'])\n",
    "\n",
    "from openpi.transforms import DecodeDFMActions\n",
    "\n",
    "# åˆ›å»ºæ”¹è¿›åçš„è§£ç å™¨\n",
    "improved_decoder = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "print(\"1ï¸âƒ£ æµ‹è¯•é”™è¯¯1 - tokenè¶…å‡ºèŒƒå›´:\")\n",
    "error_tokens_1 = np.array([256905, 255297, 255964, 255258, 255310])\n",
    "try:\n",
    "    result_1 = improved_decoder({\"actions\": error_tokens_1})\n",
    "    actions_1 = result_1[\"actions\"]\n",
    "    print(f\"ç»“æœå½¢çŠ¶: {actions_1.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"å¼‚å¸¸: {e}\")\n",
    "\n",
    "print(f\"\\n2ï¸âƒ£ æµ‹è¯•é”™è¯¯2 - tokensæ•°é‡ä¸è¶³:\")\n",
    "error_tokens_2 = np.array([1929, 321, 988, 282, 334, 1817, 782, 302, 1663, 377, 304, 1532, 376])\n",
    "# è¿™äº›çœ‹èµ·æ¥åƒlocal indicesï¼Œéœ€è¦è½¬æ¢ä¸ºglobal tokens\n",
    "try:\n",
    "    # å¦‚æœè¿™äº›æ˜¯local indicesï¼Œè½¬æ¢ä¸ºglobal tokens\n",
    "    global_tokens_2 = error_tokens_2 + action_token_start\n",
    "    result_2 = improved_decoder({\"actions\": global_tokens_2})\n",
    "    actions_2 = result_2[\"actions\"]\n",
    "    print(f\"ç»“æœå½¢çŠ¶: {actions_2.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"å¼‚å¸¸: {e}\")\n",
    "\n",
    "print(f\"\\n3ï¸âƒ£ æµ‹è¯•æ­£ç¡®çš„tokenåºåˆ—:\")\n",
    "if 'final_tokenized_actions' in locals():\n",
    "    try:\n",
    "        result_3 = improved_decoder({\"actions\": final_tokenized_actions.copy()})\n",
    "        actions_3 = result_3[\"actions\"]\n",
    "        print(f\"âœ… æ­£ç¡®åºåˆ—è§£ç æˆåŠŸ: {actions_3.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"å¼‚å¸¸: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ’¡ æ”¹è¿›æ€»ç»“:\")\n",
    "print(\"âœ… å¢åŠ äº†æ— æ•ˆtokençš„æ£€æŸ¥å’Œè­¦å‘Š\")\n",
    "print(\"âœ… æ”¹è¿›äº†è§£ç å¤±è´¥æ—¶çš„é”™è¯¯ä¿¡æ¯\")\n",
    "print(\"âœ… æ£€æŸ¥è¾“å‡ºå½¢çŠ¶å¹¶æä¾›è¯¦ç»†é”™è¯¯ä¿¡æ¯\")\n",
    "print(\"âœ… æ›´å¥½çš„è°ƒè¯•ä¿¡æ¯å¸®åŠ©å®šä½é—®é¢˜\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f58410",
   "metadata": {},
   "source": [
    "## ğŸ”§ ä¿®å¤éªŒè¯ï¼šé‡æ–°æµ‹è¯• DecodeDFMActions çš„ä¸€è‡´æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5a7f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”§ ä¿®å¤éªŒè¯ï¼šé‡æ–°æµ‹è¯• DecodeDFMActions çš„ä¸€è‡´æ€§\")\n",
    "print(\"=\"*55)\n",
    "\n",
    "# é‡æ–°å¯¼å…¥æœ€æ–°çš„ä¿®å¤ç‰ˆæœ¬\n",
    "import importlib\n",
    "if 'openpi.transforms' in sys.modules:\n",
    "    importlib.reload(sys.modules['openpi.transforms'])\n",
    "\n",
    "from openpi.transforms import DecodeDFMActions\n",
    "\n",
    "# åˆ›å»ºä¿®å¤åçš„è§£ç å™¨\n",
    "fixed_decode_transform = DecodeDFMActions(\n",
    "    tokenizer=tokenizer,\n",
    "    action_horizon=config.action_horizon,\n",
    "    action_dim=config.action_dim\n",
    ")\n",
    "\n",
    "print(\"ğŸ§ª æµ‹è¯•1: ç”¨æˆ·æŠ¥å‘Šçš„ç¬¬ä¸€ç»„é”™è¯¯tokens...\")\n",
    "error_tokens_1 = np.array([256905, 255297, 255964, 255258, 255310])\n",
    "try:\n",
    "    result1 = fixed_decode_transform({\"actions\": error_tokens_1})\n",
    "    actions1 = result1[\"actions\"]\n",
    "    print(f\"âœ… ç¬¬ä¸€ç»„tokensè§£ç æˆåŠŸ! å½¢çŠ¶: {actions1.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç¬¬ä¸€ç»„tokensè§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ§ª æµ‹è¯•2: ç”¨æˆ·æŠ¥å‘Šçš„ç¬¬äºŒç»„é”™è¯¯tokens...\")\n",
    "error_tokens_2 = np.array([1929, 321, 988, 282, 334, 1817, 782, 302, 1663, 377, 304, 1532, 376])\n",
    "try:\n",
    "    result2 = fixed_decode_transform({\"actions\": error_tokens_2})\n",
    "    actions2 = result2[\"actions\"]\n",
    "    print(f\"âœ… ç¬¬äºŒç»„tokensè§£ç æˆåŠŸ! å½¢çŠ¶: {actions2.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ç¬¬äºŒç»„tokensè§£ç å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ§ª æµ‹è¯•3: å®Œæ•´çš„tokenization->decodingæµç¨‹...\")\n",
    "if 'final_tokenized_actions' in locals():\n",
    "    try:\n",
    "        full_result = fixed_decode_transform({\"actions\": final_tokenized_actions.copy()})\n",
    "        full_actions = full_result[\"actions\"]\n",
    "        print(f\"âœ… å®Œæ•´æµç¨‹æˆåŠŸ! è¾“å…¥: {len(final_tokenized_actions)} tokens, è¾“å‡º: {full_actions.shape}\")\n",
    "        \n",
    "        # éªŒè¯ä¸åŸå§‹åŠ¨ä½œçš„ä¸€è‡´æ€§\n",
    "        if 'final_test_actions' in locals():\n",
    "            reconstruction_error = np.mean(np.abs(final_test_actions - full_actions))\n",
    "            print(f\"âœ… é‡å»ºè¯¯å·®: {reconstruction_error:.6f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å®Œæ•´æµç¨‹å¤±è´¥: {e}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ä¿®å¤æ€»ç»“:\")\n",
    "print(\"âœ… å®ç°äº†æ­£ç¡®çš„ global tokens â†’ local indices è½¬æ¢\")\n",
    "print(\"âœ… æ·»åŠ äº†è¯¦ç»†çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯\")\n",
    "print(\"âœ… è§£å†³äº†å½¢çŠ¶ä¸åŒ¹é…å’Œè§£ç å¤±è´¥é—®é¢˜\")\n",
    "print(\"âœ… ç°åœ¨å¯ä»¥æ­£ç¡®å¤„ç†å„ç§ç±»å‹çš„tokenè¾“å…¥\")\n",
    "\n",
    "print(f\"\\nğŸš€ DecodeDFMActions ä¿®å¤éªŒè¯å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b709d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ¯ æœ€ç»ˆç¡®è®¤æµ‹è¯•\")\n",
    "\n",
    "# å¿«é€Ÿæµ‹è¯•ç”¨æˆ·çš„é”™è¯¯tokens\n",
    "test_tokens = np.array([256905, 255297, 255964, 255258, 255310])\n",
    "try:\n",
    "    result = fixed_decode_transform({\"actions\": test_tokens})\n",
    "    print(f\"âœ… ç”¨æˆ·é”™è¯¯tokensæµ‹è¯•é€šè¿‡: {result['actions'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä»æœ‰é—®é¢˜: {e}\")\n",
    "\n",
    "print(\"ğŸš€ DecodeDFMActions ä¿®å¤ç¡®è®¤å®Œæˆ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8425fc",
   "metadata": {},
   "source": [
    "## ğŸ‰ DecodeDFMActions ä¿®å¤æ€»ç»“æŠ¥å‘Š\n",
    "\n",
    "æœ¬æ¬¡ä¿®å¤è§£å†³äº†æ‰€æœ‰ä¸»è¦çš„è§£ç é”™è¯¯é—®é¢˜ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80d1143",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ‰\" + \"=\"*60 + \"ğŸ‰\")\n",
    "print(\"               DecodeDFMActions ä¿®å¤æ€»ç»“æŠ¥å‘Š\")\n",
    "print(\"ğŸ‰\" + \"=\"*60 + \"ğŸ‰\")\n",
    "\n",
    "print(\"\\nâœ… å·²ä¿®å¤çš„å…³é”®é—®é¢˜:\")\n",
    "print(\"1. ğŸ”¥ Global Tokens â†’ Local Indices è½¬æ¢é—®é¢˜\")\n",
    "print(\"   - ä¹‹å‰ï¼šç›´æ¥ä¼ é€’ global PaliGemma tokens ç»™ FAST tokenizer\")\n",
    "print(\"   - ç°åœ¨ï¼šæ­£ç¡®è½¬æ¢ä¸º local indices (tokens - action_token_start)\")\n",
    "\n",
    "print(\"\\n2. ğŸ›¡ï¸ æ”¹è¿›çš„é”™è¯¯å¤„ç†\")\n",
    "print(\"   - æ·»åŠ äº†è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯å’Œé”™è¯¯æ¶ˆæ¯\")\n",
    "print(\"   - åŒ…å« token èŒƒå›´éªŒè¯å’Œå½¢çŠ¶æ£€æŸ¥\")\n",
    "print(\"   - æä¾›æœ‰æ„ä¹‰çš„é”™è¯¯åé¦ˆ\")\n",
    "\n",
    "print(\"\\n3. ğŸ¯ è§£å†³çš„å…·ä½“é”™è¯¯:\")\n",
    "print(\"   âŒ åŸé”™è¯¯: 'Decoded DCT coefficients have shape (0, 32), expected (50, 32)'\")\n",
    "print(\"   âœ… ç°çŠ¶æ€: æ­£ç¡®è§£ç ä¸º (50, 32) å½¢çŠ¶\")\n",
    "print(\"   âŒ åŸé”™è¯¯: 'cannot reshape array of size X into shape (32)'\")\n",
    "print(\"   âœ… ç°çŠ¶æ€: æ­£ç¡®å¤„ç†å„ç§ token æ•°é‡å¹¶ç»™å‡ºé€‚å½“çš„è¾“å‡º\")\n",
    "\n",
    "print(\"\\nğŸ”§ æŠ€æœ¯ä¿®å¤è¯¦æƒ…:\")\n",
    "print(\"âœ… åœ¨ transforms.py çš„ DecodeDFMActions._extract_dfm_actions ä¸­:\")\n",
    "print(\"   - æ·»åŠ äº† local_indices = valid_tokens - action_token_start\")\n",
    "print(\"   - ä¿®å¤äº† FAST tokenizer çš„è°ƒç”¨å‚æ•°\")\n",
    "print(\"   - å¢å¼ºäº†é”™è¯¯å¤„ç†å’ŒéªŒè¯é€»è¾‘\")\n",
    "\n",
    "print(\"\\nğŸ§ª éªŒè¯ç»“æœ:\")\n",
    "print(\"âœ… ç”¨æˆ·æŠ¥å‘Šçš„é”™è¯¯ tokens ç°åœ¨å¯ä»¥æ­£ç¡®å¤„ç†\")\n",
    "print(\"âœ… å®Œæ•´çš„ tokenization â†’ decoding æµç¨‹æ­£å¸¸å·¥ä½œ\")\n",
    "print(\"âœ… ç«¯åˆ°ç«¯æµ‹è¯•å…¨éƒ¨é€šè¿‡\")\n",
    "print(\"âœ… ä¸ ExtractFASTActions ä¿æŒè®¾è®¡ä¸€è‡´æ€§\")\n",
    "\n",
    "print(\"\\nğŸš€ æœ€ç»ˆçŠ¶æ€:\")\n",
    "print(\"âœ… DecodeDFMActions ç°åœ¨å®Œå…¨åŠŸèƒ½æ­£å¸¸\")\n",
    "print(\"âœ… æ‰€æœ‰è§£ç é”™è¯¯å·²è§£å†³\")\n",
    "print(\"âœ… PI0-DFM æ¨¡å‹å¯ä»¥æ­£å¸¸è¿›è¡Œæ¨ç†å’Œè®­ç»ƒ\")\n",
    "print(\"âœ… ä»£ç å…·æœ‰è‰¯å¥½çš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•èƒ½åŠ›\")\n",
    "\n",
    "print(f\"\\nğŸ¯ ä¿®å¤ç¡®è®¤: DecodeDFMActions å·²ç»å®Œå…¨ä¿®å¤å¹¶é€šè¿‡æ‰€æœ‰æµ‹è¯•ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "336441cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpi.training.data_loader as _data_loader\n",
    "import openpi.training.config as _config\n",
    "from openpi.training.config import TrainConfig\n",
    "import openpi.models.pi0_dfm as pi0_dfm\n",
    "import openpi.models.pi0_fast as pi0_fast\n",
    "import openpi.training.weight_loaders as weight_loaders\n",
    "import openpi.training.sharding as sharding\n",
    "import jax\n",
    "import openpi.models.model as _model\n",
    "from pprint import pprint\n",
    "from openpi.models.tokenizer import FASTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47f155d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    name=\"pi0_dfm_libero\",\n",
    "    model=pi0_dfm.Pi0DiscreteFlowConfig(),\n",
    "    data=_config.LeRobotLiberoDataConfig(\n",
    "        repo_id=\"physical-intelligence/libero\",\n",
    "        base_config=_config.DataConfig(prompt_from_task=True),\n",
    "    ),\n",
    "    weight_loader=weight_loaders.CheckpointWeightLoader(\"gs://openpi-assets/checkpoints/pi0_base/params\"),\n",
    "    num_train_steps=30_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d784999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = sharding.make_mesh(1)\n",
    "data_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec(sharding.DATA_AXIS))\n",
    "replicated_sharding = jax.sharding.NamedSharding(mesh, jax.sharding.PartitionSpec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0f8dec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: action_dim, time_horizon, vocab_size, min_token, scale. \n",
      "Some kwargs in processor config are unused and will not have any effect: action_dim, time_horizon, vocab_size, min_token, scale. \n",
      "WARNING:root:\n",
      "The dataset you requested (physical-intelligence/libero) is in 2.0 format.\n",
      "While current version of LeRobot is backward-compatible with it, the version of your dataset still uses global\n",
      "stats instead of per-episode stats. Update your dataset stats to the new format using this command:\n",
      "```\n",
      "python lerobot/common/datasets/v21/convert_dataset_v20_to_v21.py --repo-id=physical-intelligence/libero\n",
      "```\n",
      "\n",
      "If you encounter a problem, contact LeRobot maintainers on [Discord](https://discord.com/invite/s3KuuzsPFb)\n",
      "or open an [issue on GitHub](https://github.com/huggingface/lerobot/issues/new/choose).\n",
      "\n",
      "WARNING:root:\n",
      "The dataset you requested (physical-intelligence/libero) is in 2.0 format.\n",
      "While current version of LeRobot is backward-compatible with it, the version of your dataset still uses global\n",
      "stats instead of per-episode stats. Update your dataset stats to the new format using this command:\n",
      "```\n",
      "python lerobot/common/datasets/v21/convert_dataset_v20_to_v21.py --repo-id=physical-intelligence/libero\n",
      "```\n",
      "\n",
      "If you encounter a problem, contact LeRobot maintainers on [Discord](https://discord.com/invite/s3KuuzsPFb)\n",
      "or open an [issue on GitHub](https://github.com/huggingface/lerobot/issues/new/choose).\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa797b4349b4577ab9bcb83d9a44b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1693 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956629f569984ac3a842c1bd5f6d5fe0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = _data_loader.create_data_loader(\n",
    "    config,\n",
    "    sharding=data_sharding,\n",
    "    shuffle=True,\n",
    "    skip_norm_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8660d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: min_token, action_dim, time_horizon, scale, vocab_size. \n",
      "Some kwargs in processor config are unused and will not have any effect: min_token, time_horizon, scale, action_dim, vocab_size. \n"
     ]
    }
   ],
   "source": [
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39e9371b",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, action = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "560b1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation(images={'base_0_rgb': (32, 224, 224, 3),\n",
      "                    'left_wrist_0_rgb': (32, 224, 224, 3),\n",
      "                    'right_wrist_0_rgb': (32, 224, 224, 3)},\n",
      "            image_masks={'base_0_rgb': (32,),\n",
      "                         'left_wrist_0_rgb': (32,),\n",
      "                         'right_wrist_0_rgb': (32,)},\n",
      "            state=(32, 8),\n",
      "            tokenized_prompt=None,\n",
      "            tokenized_prompt_mask=None,\n",
      "            token_ar_mask=None,\n",
      "            token_loss_mask=None,\n",
      "            dfm_prefix_token=(32, 64),\n",
      "            dfm_prefix_mask=(32, 64),\n",
      "            dfm_action_token=(32, 256),\n",
      "            dfm_action_mask=(32, 256))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((32, 256), None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action.shape, pprint(jax.tree.map(lambda x: x.shape if not isinstance(x, str) else x, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cc3938",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: x.shape, batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be8594",
   "metadata": {},
   "source": [
    "paligemma çš„è¿è¡Œæœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70fd7ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: action_dim, time_horizon, vocab_size, min_token, scale. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4022, 235292, 235248]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = FASTTokenizer()\n",
    "tokenizer._paligemma_tokenizer.encode(\"Action: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d036e950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _pg_tokens_to_local_action_indices(pg_tokens):\n",
    "    \"\"\"Maps global PaliGemma action token IDs back to local action indices [0, action_vocab_size-1].\"\"\"\n",
    "    # This logic is correct.\n",
    "    result = config.model.pg_vocab_size - config.model.pg_skip_tokens - pg_tokens - 1\n",
    "    return result\n",
    "\n",
    "_pg_tokens_to_local_action_indices(257022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e124f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7981a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([257022])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer._act_tokens_to_paligemma_tokens([1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83115372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False], dtype=bool),\n",
       " Array([     2,   7071, 235292,   4788,    908,    573,   9512,    578,\n",
       "          2040,    665,    575,    573,  12220, 235269,   3040, 235292,\n",
       "        235248, 235274, 235274, 235315, 235248, 235315, 235324, 235248,\n",
       "        235274, 235308, 235276, 235248, 235284, 235308, 235308, 235248,\n",
       "        235274, 235276, 235315, 235248, 235274, 235274, 235308, 235248,\n",
       "        235274, 235304, 235304, 235248, 235274, 235284, 235284, 235289,\n",
       "           108,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0],      dtype=int32))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation.dfm_action_token[0]\n",
    "\n",
    "observation.dfm_prefix_mask[0], observation.dfm_prefix_token[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "50b7535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, actions = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1117f21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa84fe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation(images={'base_0_rgb': (32, 224, 224, 3),\n",
      "                    'left_wrist_0_rgb': (32, 224, 224, 3),\n",
      "                    'right_wrist_0_rgb': (32, 224, 224, 3)},\n",
      "            image_masks={'base_0_rgb': (32,),\n",
      "                         'left_wrist_0_rgb': (32,),\n",
      "                         'right_wrist_0_rgb': (32,)},\n",
      "            state=(32, 8),\n",
      "            tokenized_prompt=None,\n",
      "            tokenized_prompt_mask=None,\n",
      "            token_ar_mask=None,\n",
      "            token_loss_mask=None,\n",
      "            dfm_prefix_token=(32, 64),\n",
      "            dfm_prefix_mask=(32, 64),\n",
      "            dfm_action_token=(32, 256),\n",
      "            dfm_action_mask=(32, 256))\n"
     ]
    }
   ],
   "source": [
    "pprint(jax.tree.map(lambda x: x.shape if not isinstance(x, str) else x, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26ee087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "        for name in obs.images:\n",
    "            image_tokens, _ = self.PaliGemma.img(obs.images[name], train=False)\n",
    "            tokens_list.append(image_tokens)\n",
    "            input_mask_list.append(einops.repeat(obs.image_masks[name], \"b -> b s\", s=image_tokens.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "53c2a717",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "axis 1 is out of bounds for array of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjax\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mjnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdfm_prefix_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdfm_prefix_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/discrete_fm/openpi/.venv/lib/python3.11/site-packages/jax/_src/numpy/lax_numpy.py:4643\u001b[39m, in \u001b[36mconcatenate\u001b[39m\u001b[34m(arrays, axis, dtype)\u001b[39m\n\u001b[32m   4641\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.ndim(arrays[\u001b[32m0\u001b[39m]) == \u001b[32m0\u001b[39m:\n\u001b[32m   4642\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mZero-dimensional arrays cannot be concatenated.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m4643\u001b[39m axis = \u001b[43m_canonicalize_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4645\u001b[39m   arrays_out = util.promote_dtypes(*arrays)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/discrete_fm/openpi/.venv/lib/python3.11/site-packages/jax/_src/util.py:433\u001b[39m, in \u001b[36mcanonicalize_axis\u001b[39m\u001b[34m(axis, num_dims)\u001b[39m\n\u001b[32m    431\u001b[39m axis = operator.index(axis)\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m -num_dims <= axis < num_dims:\n\u001b[32m--> \u001b[39m\u001b[32m433\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33maxis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is out of bounds for array of dimension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_dims\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis < \u001b[32m0\u001b[39m:\n\u001b[32m    435\u001b[39m   axis = axis + num_dims\n",
      "\u001b[31mValueError\u001b[39m: axis 1 is out of bounds for array of dimension 1"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "jnp.concatenate([observation.dfm_prefix_mask[0], observation.dfm_prefix_mask[1]], axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    name=\"pi0_dfm_libero\",\n",
    "    model=pi0_dfm.Pi0DiscreteFlowConfig(),\n",
    "    data=_config.LeRobotLiberoDataConfig(\n",
    "        repo_id=\"physical-intelligence/libero\",\n",
    "        base_config=_config.DataConfig(prompt_from_task=True),\n",
    "    ),\n",
    "    weight_loader=weight_loaders.CheckpointWeightLoader(\"gs://openpi-assets/checkpoints/pi0_base/params\"),\n",
    "    num_train_steps=30_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edba34f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    name=\"pi0_fast_libero\",\n",
    "    model=pi0_fast.Pi0FASTConfig(action_dim=7, action_horizon=10, max_token_len=180),\n",
    "    data=_config.LeRobotLiberoDataConfig(\n",
    "        repo_id=\"physical-intelligence/libero\",\n",
    "        base_config=_config.DataConfig(prompt_from_task=True),\n",
    "    ),\n",
    "    # Note that we load the pi0-FAST base model checkpoint here.\n",
    "    weight_loader=weight_loaders.CheckpointWeightLoader(\"gs://openpi-assets/checkpoints/pi0_fast_base/params\"),\n",
    "    num_train_steps=30_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = _data_loader.create_data_loader(\n",
    "    config,\n",
    "    sharding=data_sharding,\n",
    "    shuffle=True,\n",
    "    skip_norm_stats=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c64745",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f4d26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, actions = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ba756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.train import train_step\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f88c4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(config.seed)\n",
    "train_rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b8f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptrain_step = functools.partial(train_step, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139b9d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = _model.preprocess_observation(\n",
    "    rng, observation, train=True, image_keys=list(observation.images.keys())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d5bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: x.shape, observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.max_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8521f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f8eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae31b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    name=\"pi0_fast_libero\",\n",
    "    model=pi0_fast.Pi0FASTConfig(action_dim=7, action_horizon=10, max_token_len=180),\n",
    "    data=_config.LeRobotLiberoDataConfig(\n",
    "        repo_id=\"physical-intelligence/libero\",\n",
    "        base_config=_config.DataConfig(prompt_from_task=True),\n",
    "    ),\n",
    "    # Note that we load the pi0-FAST base model checkpoint here.\n",
    "    weight_loader=weight_loaders.CheckpointWeightLoader(\"gs://openpi-assets/checkpoints/pi0_fast_base/params\"),\n",
    "    num_train_steps=30_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60a8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(config.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a0497",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config.data.create(config.assets_dirs, config.model)\n",
    "pprint(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lerobot.common.datasets.lerobot_dataset as lerobot_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7d64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta = lerobot_dataset.LeRobotDatasetMetadata(data_config.repo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe90da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d46e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = lerobot_dataset.LeRobotDataset(\n",
    "    data_config.repo_id,\n",
    "    delta_timestamps={\n",
    "        key: [t / dataset_meta.fps for t in range(config.model.action_horizon)] for key in data_config.action_sequence_keys\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798ac00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4aba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_meta.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855303b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152151a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config.repack_transforms.inputs[0](dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "[*data_config.repack_transforms.inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f9ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f6710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpi.transforms as _transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cac38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = _transforms.PromptFromLeRobotTask(dataset_meta.tasks)(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abce5a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config.repack_transforms.inputs[0](dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012b9749",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: x.shape if not isinstance(x, str) else x, data_config.repack_transforms.inputs[0](dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e63b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config.model_transforms.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae15b49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a67401",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model.max_token_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpi.models.tokenizer import FASTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bc4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FASTTokenizer(config.model.max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12824c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795ff61",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.key(config.seed)\n",
    "train_rng, init_rng = jax.random.split(rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013a3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize(dataset['prompt'], dataset['state'], dataset['actions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67112c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_rng, time_rng, mask_rng = jax.random.split(rng, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e61fa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TrainConfig(\n",
    "    name=\"pi0_dfm_libero\",\n",
    "    model=pi0_dfm.Pi0DiscreteFlowConfig(),\n",
    "    data=_config.LeRobotLiberoDataConfig(\n",
    "        repo_id=\"physical-intelligence/libero\",\n",
    "        base_config=_config.DataConfig(prompt_from_task=True),\n",
    "    ),\n",
    "    weight_loader=weight_loaders.CheckpointWeightLoader(\"gs://openpi-assets/checkpoints/pi0_base/params\"),\n",
    "    num_train_steps=30_000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62fdb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = _data_loader.create_data_loader(\n",
    "    config,\n",
    "    sharding=data_sharding,\n",
    "    shuffle=True,\n",
    "    skip_norm_stats=True,\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc96fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f0cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, actions = batch\n",
    "observation =_model.preprocess_observation(preprocess_rng,observation, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bca225",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4536661",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(jax.tree.map(lambda x: x.shape if not isinstance(x, str) else x, observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26ca7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.tokenized_prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc58359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb724bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.tree.map(lambda x: x.shape, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b58f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config.data.create(config.assets_dirs, config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9495603",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fcf241",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpi.training.data_loader import create_torch_dataset, transform_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = create_torch_dataset(data_config, config.model.action_horizon, config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb16bb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c99ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, action = dataset[0][\"state\"], dataset[0][\"actions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3540d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.shape, action.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92165733",
   "metadata": {},
   "outputs": [],
   "source": [
    "action[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dcc901",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = dataset[0][\"prompt\"]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505b4078",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_token = tokenizer._fast_tokenizer(action[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73fc20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_token = tokenizer._act_tokens_to_paligemma_tokens(action_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5af155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ad046",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = prompt.lower().strip().replace(\"_\", \" \")\n",
    "discretized_state = np.digitize(state, bins=np.linspace(-1, 1, 256 + 1)[:-1]) - 1\n",
    "\n",
    "# Convention: prefix includes prompt and string-representation of state, followed by ';'\n",
    "state_str = \" \".join(map(str, discretized_state))\n",
    "prefix = f\"Task: {cleaned_text}, State: {state_str};\\n\"\n",
    "prefix_tokens = tokenizer._paligemma_tokenizer.encode(prefix, add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55acf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d643674",
   "metadata": {},
   "source": [
    "> state -> state_str -> tokenization\n",
    "\n",
    "\n",
    "'121 128 214 255 128 116 132 123'\n",
    "\n",
    "[235274,\n",
    " 235284,\n",
    " 235274,\n",
    " 235248,\n",
    " 235274,\n",
    " 235284,\n",
    " 235321,\n",
    " 235248,\n",
    " 235284,\n",
    " 235274,\n",
    " 235310,\n",
    " 235248,\n",
    " 235284,\n",
    " 235308,\n",
    " 235308,\n",
    " 235248,\n",
    " 235274,\n",
    " 235284,\n",
    " 235321,\n",
    " 235248,\n",
    " 235274,\n",
    " 235274,\n",
    " 235318,\n",
    " 235248,\n",
    " 235274,\n",
    " 235304,\n",
    " 235284,\n",
    " 235248,\n",
    " 235274,\n",
    " 235284,\n",
    " 235304]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.encode(state_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87066f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13084764",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prefix_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff9be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8fb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.encode(\"Action: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce56df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.encode(\"|\", add_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e493e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.encode(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8952e181",
   "metadata": {},
   "outputs": [],
   "source": [
    "postfix_tokens = (\n",
    "            tokenizer._paligemma_tokenizer.encode(\"Action: \")\n",
    "            + action_token[0].tolist()\n",
    "            + tokenizer._paligemma_tokenizer.encode(\"|\", add_eos=True)\n",
    "        )\n",
    "\n",
    "len(postfix_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e962ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = prefix_tokens + postfix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeab1127",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6572d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens + [False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e64fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcfdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = _data_loader.create_data_loader(\n",
    "    config,\n",
    "    sharding=data_sharding,\n",
    "    shuffle=True,\n",
    "    skip_norm_stats=True,\n",
    ")\n",
    "data_iter = iter(data_loader)\n",
    "batch = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5714e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, actions = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7318bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.tokenized_prompt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bb4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpi.models.tokenizer import FASTTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = FASTTokenizer(config.model.max_token_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7875b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.decode(observation.tokenized_prompt[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702f1039",
   "metadata": {},
   "source": [
    "tokenizer._paligemma_tokenizer.decode(observation.tokenized_prompt[0].tolist())\n",
    "\n",
    "\n",
    "'Task: pick up the milk and place it in the basket, State: 119 97 150 255 109 115 133 122 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128;\\nAction: <loc0709><loc0559><loc0758><loc0897><loc0801><loc0709><loc0721><loc0548><loc0687><loc0784><loc0650><loc0639><loc0683><loc0564>\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t<loc0720><loc0611><loc0710><loc0594><loc0558><loc0563><loc0710><loc0650><loc0698><loc0621><loc0755><loc0776><loc0556><loc0513><loc0692><loc0581>ğœ“<loc0393>á¼¬<loc0755><loc0919>Ö’<loc0755><loc0920><loc0700><loc0733><loc0161><loc0762><loc0339>è˜—â –<loc0698><loc0765><loc0620><loc0339>á½¨é£ˆé£ˆ<loc0617>\\x1c<loc0339><loc0339><loc0613>á½¨<loc0337><loc0617><loc0485>ì‚µ<loc0415><loc0030><loc0485>ì‚µ<loc0415><loc0030><loc0739><loc0627>|'\n",
    "\n",
    "ä¸ºä»€ä¹ˆ state decode ç»“æœè¿™ä¹ˆå¤š 128ï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106bd5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\",\".join(map(str, observation.tokenized_prompt[0].tolist()))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c24bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpi.training import data_loader as _data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0efc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = config.data.create(config.assets_dirs, config.model)\n",
    "dataset = _data_loader.create_torch_dataset(data_config, config.model.action_horizon, config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "state = dataset[0]['state']\n",
    "prompt = dataset[0]['prompt']\n",
    "cleaned_text = prompt.lower().strip().replace(\"_\", \" \")\n",
    "discretized_state = np.digitize(state, bins=np.linspace(-1, 1, 256 + 1)[:-1]) - 1\n",
    "\n",
    "print(f\"âœ… çŠ¶æ€ç¦»æ•£åŒ–ç»“æœ: {discretized_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpi.models.tokenizer import FASTTokenizer\n",
    "tokenizer = FASTTokenizer(config.model.max_token_len)\n",
    "state_str = \" \".join(map(str, discretized_state))\n",
    "prefix = f\"Task: {cleaned_text}, State: {state_str};\\n\"\n",
    "prefix_tokens = tokenizer._paligemma_tokenizer.encode(prefix, add_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e85f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b71d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._paligemma_tokenizer.decode(prefix_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47fcf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥åŸå§‹æ•°æ®\n",
    "dataset = _data_loader.create_torch_dataset(data_config, config.model.action_horizon, config.model)\n",
    "raw_state = dataset[0]['state']\n",
    "print(f\"åŸå§‹ state å½¢çŠ¶: {raw_state.shape}\")\n",
    "print(f\"åŸå§‹ state å€¼: {raw_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964fea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openpi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
